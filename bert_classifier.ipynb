{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "#Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
    "\n",
    "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
    "\n",
    "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d31e1ccd4e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jviywGyWyKsA",
    "outputId": "166f3005-d219-404f-b201-2a0b75480360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 28.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow_hub\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 37.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert-tensorflow) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (1.16.4)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (3.7.1)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub) (41.4.0)\n",
      "Installing collected packages: bert-tensorflow, tensorflow-hub\n",
      "Successfully installed bert-tensorflow-1.0.1 tensorflow-hub-0.7.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "7780a032-31d4-4794-e6aa-664a5d2ae7dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: output *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = 'output'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = True #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "34445cb8-2be0-4379-fdbc-7794091f6049"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'target'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, train_features, pred_features, train=True):\n",
    "        if train:\n",
    "            self.feature_names = train_features\n",
    "        else:\n",
    "            self.feature_names = pred_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.feature_names]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "URL_PATTERN = re.compile(r\"(https:\\/\\/\\S+)|(http:\\/\\/\\S+)|(www\\.\\S+)\")\n",
    "HTML_TAGS_PATTERN = re.compile(r'<.*>')\n",
    "ALPHA_NUMERIC_PATTERN = re.compile(r\"\\w*[:,-]*(\\d+[:,-]*)+\\d*\\w*\")\n",
    "PUNCTUATION_PATTERN = re.compile(r'[^a-zA-Z ]')\n",
    "MENTIONS_PATTERN = re.compile(r'@[\\w]*')\n",
    "HASH_TAGS_PATTERN = re.compile(r'#\\S+')\n",
    "UNWANTED_WORDS_PATTERN = re.compile(r'&amp;|RT: \\S+:|RT \\S+:|FYI|CAD|RT |GMT|UTC|JST|\\s[b-zB-Z]\\s|ST|nsfw')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "class Preprocess(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, remove_stop_words=False):\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "    \n",
    "    def to_lower(self, tweet):\n",
    "        return tweet.lower()\n",
    "    \n",
    "    def replace_urls(self, tweet):\n",
    "        return URL_PATTERN.sub('', tweet)\n",
    "    \n",
    "    def replace_html(self, tweet):\n",
    "        return HTML_TAGS_PATTERN.sub(''. tweet)\n",
    "    \n",
    "    def replace_non_alpha(self, tweet):\n",
    "        return ALPHA_NUMERIC_PATTERN.sub('', tweet)\n",
    "    \n",
    "    def replace_punctuations(self, tweet):\n",
    "        return PUNCTUATION_PATTERN.sub('', tweet)\n",
    "    \n",
    "    def replace_mentions(self, tweet):\n",
    "        return MENTIONS_PATTERN.sub('', tweet)\n",
    "    \n",
    "    def replace_hashtags(self, tweet):\n",
    "        return HASH_TAGS_PATTERN.sub('', tweet)\n",
    "    \n",
    "    def replace_unwanted_words(self, tweet):\n",
    "        return UNWANTED_WORDS_PATTERN.sub('', tweet)\n",
    "    \n",
    "    def remove_all_stop_words(self, tweet):\n",
    "        word_tokens = word_tokenize(tweet) \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        return ' '.join(filtered_sentence)\n",
    "    \n",
    "    def tweet_transformer(self, tweet):\n",
    "        tweet = self.to_lower(tweet)\n",
    "        tweet = self.replace_urls(tweet)\n",
    "        tweet = self.replace_non_alpha(tweet)\n",
    "        tweet = self.replace_punctuations(tweet)\n",
    "        tweet = self.replace_mentions(tweet)\n",
    "        tweet = self.replace_hashtags(tweet)\n",
    "        tweet = self.replace_unwanted_words(tweet)\n",
    "        if self.remove_stop_words:\n",
    "            tweet = self.remove_all_stop_words(tweet) \n",
    "        return tweet\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cleaned = X['text'].apply(lambda tweet: self.tweet_transformer(tweet))\n",
    "#         a.to_frame().join(b)\n",
    "        #return cleaned.to_frame().join(X['target'])\n",
    "        return cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "0     Our Deeds are the Reason of this #earthquake M...\n",
      "1                Forest fire near La Ronge Sask. Canada\n",
      "2     All residents asked to 'shelter in place' are ...\n",
      "3     13,000 people receive #wildfires evacuation or...\n",
      "4     Just got sent this photo from Ruby #Alaska as ...\n",
      "5     #RockyFire Update => California Hwy. 20 closed...\n",
      "6     #flood #disaster Heavy rain causes flash flood...\n",
      "7     I'm on top of the hill and I can see a fire in...\n",
      "8     There's an emergency evacuation happening now ...\n",
      "9     I'm afraid that the tornado is coming to our a...\n",
      "10          Three people died from the heat wave so far\n",
      "11    Haha South Tampa is getting flooded hah- WAIT ...\n",
      "12    #raining #flooding #Florida #TampaBay #Tampa 1...\n",
      "13              #Flood in Bago Myanmar #We arrived Bago\n",
      "14    Damage to school bus on 80 in multi car crash ...\n",
      "15                                       What's up man?\n",
      "16                                        I love fruits\n",
      "17                                     Summer is lovely\n",
      "18                                    My car is so fast\n",
      "19                         What a goooooooaaaaaal!!!!!!\n",
      "20                               this is ridiculous....\n",
      "21                                    London is cool ;)\n",
      "22                                          Love skiing\n",
      "23                                What a wonderful day!\n",
      "24                                             LOOOOOOL\n",
      "25                       No way...I can't eat that shit\n",
      "26                                Was in NYC last week!\n",
      "27                                   Love my girlfriend\n",
      "28                                            Cooool :)\n",
      "29                                   Do you like pasta?\n",
      "...                                                 ...\n",
      "7583  Pic of 16yr old PKK suicide bomber who detonat...\n",
      "7584  These boxes are ready to explode! Exploding Ki...\n",
      "7585  Calgary Police Flood Road Closures in Calgary....\n",
      "7586  #Sismo DETECTADO #JapÌ_n 15:41:07 Seismic inte...\n",
      "7587                                 Sirens everywhere!\n",
      "7588  BREAKING: #ISIS claims responsibility for mosq...\n",
      "7589                                     Omg earthquake\n",
      "7590  SEVERE WEATHER BULLETIN No. 5 FOR: TYPHOON ÛÏ...\n",
      "7591  Heat wave warning aa? Ayyo dei. Just when I pl...\n",
      "7592  An IS group suicide bomber detonated an explos...\n",
      "7593  I just heard a really loud bang and everyone i...\n",
      "7594  A gas thing just exploded and I heard screams ...\n",
      "7595  NWS: Flash Flood Warning Continued for Shelby ...\n",
      "7596  RT @LivingSafely: #NWS issues Severe #Thunders...\n",
      "7597  #??? #?? #??? #??? MH370: Aircraft debris foun...\n",
      "7598  Father-of-three Lost Control of Car After Over...\n",
      "7599  1.3 #Earthquake in 9Km Ssw Of Anza California ...\n",
      "7600  Evacuation order lifted for town of Roosevelt:...\n",
      "7601  #breaking #LA Refugio oil spill may have been ...\n",
      "7602  a siren just went off and it wasn't the Forney...\n",
      "7603  Officials say a quarantine is in place at an A...\n",
      "7604  #WorldNews Fallen powerlines on G:link tram: U...\n",
      "7605  on the flip side I'm at Walmart and there is a...\n",
      "7606  Suicide bomber kills 15 in Saudi security site...\n",
      "7607  #stormchase Violent Record Breaking EF-5 El Re...\n",
      "7608  Two giant cranes holding a bridge collapse int...\n",
      "7609  @aria_ahrary @TheTawniest The out of control w...\n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
      "7611  Police investigating after an e-bike collided ...\n",
      "7612  The Latest: More Homes Razed by Northern Calif...\n",
      "\n",
      "[7613 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['text', 'target'], ['text'], True)),\n",
    "    ('preprocess', Preprocess(True))\n",
    "])\n",
    "\n",
    "train_cleaned = pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('target', 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4380\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d5e2124006c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                    \u001b[0mtext_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                                    \u001b[0mtext_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                                                    label = x[LABEL_COLUMN]), axis = 1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-d5e2124006c8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                    \u001b[0mtext_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                                    \u001b[0mtext_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                                                    label = x[LABEL_COLUMN]), axis = 1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('target', 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bert.run_classifier.InputExample object at 0x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;bert.run_classifier.InputExample object at 0x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bert.run_classifier.InputExample object at 0x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bert.run_classifier.InputExample object at 0x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bert.run_classifier.InputExample object at 0x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  <bert.run_classifier.InputExample object at 0x...\n",
       "1  <bert.run_classifier.InputExample object at 0x...\n",
       "2  <bert.run_classifier.InputExample object at 0x...\n",
       "3  <bert.run_classifier.InputExample object at 0x...\n",
       "4  <bert.run_classifier.InputExample object at 0x..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "pd.DataFrame(train_InputExamples).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "20b28cc7-3cb3-4ce6-bfff-a7847ce3bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "dsBo6RCtQmwx",
    "outputId": "9af8c917-90ec-4fe9-897b-79dc89ca88e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "65001dda-155b-48fc-b5fc-1e4cabc8dfbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i ' m watching this on the sci - fi channel right now . it ' s so horrible i can ' t stop watching it ! i ' m a video ##grapher and this movie makes me sad . i feel bad for anyone associated with this movie . some of the camera work is good . most is very questionable . there are a few decent actors in the flick . too bad they ' re surrounded by what must have been the director ' s relatives . that ' s the only way they could have been qualified to be in a movie ! music was a little better than the acting . if you get around to watching this i hope it [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 1005 1049 3666 2023 2006 1996 16596 1011 10882 3149 2157 2085 1012 2009 1005 1055 2061 9202 1045 2064 1005 1056 2644 3666 2009 999 1045 1005 1049 1037 2678 18657 1998 2023 3185 3084 2033 6517 1012 1045 2514 2919 2005 3087 3378 2007 2023 3185 1012 2070 1997 1996 4950 2147 2003 2204 1012 2087 2003 2200 21068 1012 2045 2024 1037 2261 11519 5889 1999 1996 17312 1012 2205 2919 2027 1005 2128 5129 2011 2054 2442 2031 2042 1996 2472 1005 1055 9064 1012 2008 1005 1055 1996 2069 2126 2027 2071 2031 2042 4591 2000 2022 1999 1037 3185 999 2189 2001 1037 2210 2488 2084 1996 3772 1012 2065 2017 2131 2105 2000 3666 2023 1045 3246 2009 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i have been a fan of pushing dai ##sies since the very beginning . it is wonderful ##ly thought up , and bryan fuller has the most remarkable ideas for this show . < br / > < br / > it is unbelievable on how much tv has been needing a creative , original show like pushing dai ##sies . it is a huge relief to see a show , that is unlike the rest , where as , if you compared it to some of the newer shows , such as scrub ##s and house , you would see the similarities , and it does get ted ##ious at moments to see shows so close in identity . < br / > < br [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2031 2042 1037 5470 1997 6183 18765 14625 2144 1996 2200 2927 1012 2009 2003 6919 2135 2245 2039 1010 1998 8527 12548 2038 1996 2087 9487 4784 2005 2023 2265 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 2003 23653 2006 2129 2172 2694 2038 2042 11303 1037 5541 1010 2434 2265 2066 6183 18765 14625 1012 2009 2003 1037 4121 4335 2000 2156 1037 2265 1010 2008 2003 4406 1996 2717 1010 2073 2004 1010 2065 2017 4102 2009 2000 2070 1997 1996 10947 3065 1010 2107 2004 18157 2015 1998 2160 1010 2017 2052 2156 1996 12319 1010 1998 2009 2515 2131 6945 6313 2012 5312 2000 2156 3065 2061 2485 1999 4767 1012 1026 7987 1013 1028 1026 7987 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] this movie starts out promising ##ly , with an early scene in which frank morgan advises against gary cooper ' s marriage to his daughter , anita louise . frank morgan , playing an una ##bas ##hed gold - digger , loudly complain ##s to cooper about his perceived pen ##ury at the hands of his family - including his daughter , anita louise . i am a fan of all 3 actors . frank morgan is ( to my mind ) a hollywood treasure , cooper a legend , and louise a very lovely , versatile and under - appreciated actress seldom seen in the leading role . i also have nothing against teresa wright , and while not blessed with great range , she [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2023 3185 4627 2041 10015 2135 1010 2007 2019 2220 3496 1999 2029 3581 5253 25453 2114 5639 6201 1005 1055 3510 2000 2010 2684 1010 12918 8227 1012 3581 5253 1010 2652 2019 14477 22083 9072 2751 1011 28661 1010 9928 17612 2015 2000 6201 2055 2010 8690 7279 13098 2012 1996 2398 1997 2010 2155 1011 2164 2010 2684 1010 12918 8227 1012 1045 2572 1037 5470 1997 2035 1017 5889 1012 3581 5253 2003 1006 2000 2026 2568 1007 1037 5365 8813 1010 6201 1037 5722 1010 1998 8227 1037 2200 8403 1010 22979 1998 2104 1011 12315 3883 15839 2464 1999 1996 2877 2535 1012 1045 2036 2031 2498 2114 12409 6119 1010 1998 2096 2025 10190 2007 2307 2846 1010 2016 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i was over ##taken by the emotion . un ##for ##get ##table rendering of a wartime story which is unknown to most people . the performances were fault ##less and outstanding . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2001 2058 25310 2011 1996 7603 1012 4895 29278 18150 10880 14259 1997 1037 12498 2466 2029 2003 4242 2000 2087 2111 1012 1996 4616 2020 6346 3238 1998 5151 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] soldier blue is a movie with pre ##tension ##s : pre ##tension ##s to be some sort of profound statement on man ' s inhuman ##ity to man , on the white man ' s exploitation of and brutality towards indigenous peoples ; a biting , un ##fl ##in ##ching and sar ##don ##ic commentary on the horrors of vietnam . well , sorry , but it fails mis ##era ##bly to be any of those things . what soldier blue actually is is per ##nic ##ious , tri ##te , badly made , dish ##ones ##t rubbish . < br / > < br / > another reviewer here hit the nail on the head in saying that it appears to be a hybrid of [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5268 2630 2003 1037 3185 2007 3653 29048 2015 1024 3653 29048 2015 2000 2022 2070 4066 1997 13769 4861 2006 2158 1005 1055 29582 3012 2000 2158 1010 2006 1996 2317 2158 1005 1055 14427 1997 1998 24083 2875 6284 7243 1025 1037 12344 1010 4895 10258 2378 8450 1998 18906 5280 2594 8570 2006 1996 22812 1997 5148 1012 2092 1010 3374 1010 2021 2009 11896 28616 6906 6321 2000 2022 2151 1997 2216 2477 1012 2054 5268 2630 2941 2003 2003 2566 8713 6313 1010 13012 2618 1010 6649 2081 1010 9841 21821 2102 29132 1012 1026 7987 1013 1028 1026 7987 1013 1028 2178 12027 2182 2718 1996 13774 2006 1996 2132 1999 3038 2008 2009 3544 2000 2022 1037 8893 1997 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:Writing example 0 of 5000\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i just watched this today on tv . it was on abc ' s sunday afternoon movie . < br / > < br / > this wasn ' t a very good movie , but for a low budget independent film like this , it was okay . there is some suspense in it , but there are so many bad qualities that really bring the movie down . the script is pretty lame , and the plot elements aren ' t very realistic , such as the way a 911 operator would laugh and hang up when someone is reporting a murder . i don ' t know what the writer was thinking when they came up with that idea , but it isn [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2074 3427 2023 2651 2006 2694 1012 2009 2001 2006 5925 1005 1055 4465 5027 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2347 1005 1056 1037 2200 2204 3185 1010 2021 2005 1037 2659 5166 2981 2143 2066 2023 1010 2009 2001 3100 1012 2045 2003 2070 23873 1999 2009 1010 2021 2045 2024 2061 2116 2919 11647 2008 2428 3288 1996 3185 2091 1012 1996 5896 2003 3492 20342 1010 1998 1996 5436 3787 4995 1005 1056 2200 12689 1010 2107 2004 1996 2126 1037 19989 6872 2052 4756 1998 6865 2039 2043 2619 2003 7316 1037 4028 1012 1045 2123 1005 1056 2113 2054 1996 3213 2001 3241 2043 2027 2234 2039 2007 2008 2801 1010 2021 2009 3475 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] from hardly alien sounding lasers , to an elementary school style shuttle crash , \" night ##be ##ast \" is better classified as a far ##cic ##al mix of fake blood and bare chest . the almost pornographic style of the film seems to be a failed attempt to recover from a lack of co ##hesive or effective story . the acting however is not nearly as beast ##ly , many of the young , aspiring , actors ad ##mir ##ably showcase a hidden talent . particularly don lei ##fer ##t and jamie ze ##mare ##l , who shed a well needed sha ##rd of light on this otherwise terrible film . night ##be ##ast would have never shown up on set had he known the [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2013 6684 7344 9391 23965 1010 2000 2019 4732 2082 2806 10382 5823 1010 1000 2305 4783 14083 1000 2003 2488 6219 2004 1037 2521 19053 2389 4666 1997 8275 2668 1998 6436 3108 1012 1996 2471 26932 2806 1997 1996 2143 3849 2000 2022 1037 3478 3535 2000 8980 2013 1037 3768 1997 2522 21579 2030 4621 2466 1012 1996 3772 2174 2003 2025 3053 2004 6841 2135 1010 2116 1997 1996 2402 1010 22344 1010 5889 4748 14503 8231 13398 1037 5023 5848 1012 3391 2123 26947 7512 2102 1998 6175 27838 24376 2140 1010 2040 8328 1037 2092 2734 21146 4103 1997 2422 2006 2023 4728 6659 2143 1012 2305 4783 14083 2052 2031 2196 3491 2039 2006 2275 2018 2002 2124 1996 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] here we have the in ##imi ##table charlie chaplin for ##sa ##king his slap ##stick past to tackle the serious subject of anti - semi ##tism , and into ##ler ##ance in general . he portrays two characters - the sweet , innocent jewish barber - a war veteran , and the ravi ##ng and ruthless dictator , aden ##oid h ##yn ##kel . the jewish ghetto in this country is not safe for long , due to the w ##him ##s of h ##yn ##kel and his armed thugs , who routinely rough up its residents , or leave them alone , dependent upon his mood that day or week . the barber is among them , but is befriended by his former commanding officer [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2182 2057 2031 1996 1999 27605 10880 4918 23331 2005 3736 6834 2010 14308 21354 2627 2000 11147 1996 3809 3395 1997 3424 1011 4100 17456 1010 1998 2046 3917 6651 1999 2236 1012 2002 17509 2048 3494 1011 1996 4086 1010 7036 3644 13362 1011 1037 2162 8003 1010 1998 1996 16806 3070 1998 18101 21237 1010 16298 9314 1044 6038 11705 1012 1996 3644 17276 1999 2023 2406 2003 2025 3647 2005 2146 1010 2349 2000 1996 1059 14341 2015 1997 1044 6038 11705 1998 2010 4273 24106 1010 2040 19974 5931 2039 2049 3901 1010 2030 2681 2068 2894 1010 7790 2588 2010 6888 2008 2154 2030 2733 1012 1996 13362 2003 2426 2068 1010 2021 2003 23386 2011 2010 2280 7991 2961 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i really hated this movie and it ' s the first movie written by stephen king that i didn ' t finish . i was truly disappointed , it was the worst crap i ' ve ever seen . what were you thinking making three hours out of it ? it may have a quite good story , but actors ? no . suspense ? no . romance ? no . horror ? no . it didn ' t have anything . < br / > < br / > it ' s got this strange , crazy science man with einstein - hair , the classic thing . not real at all . and a man keep getting younger all the time . it seems [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2428 6283 2023 3185 1998 2009 1005 1055 1996 2034 3185 2517 2011 4459 2332 2008 1045 2134 1005 1056 3926 1012 1045 2001 5621 9364 1010 2009 2001 1996 5409 10231 1045 1005 2310 2412 2464 1012 2054 2020 2017 3241 2437 2093 2847 2041 1997 2009 1029 2009 2089 2031 1037 3243 2204 2466 1010 2021 5889 1029 2053 1012 23873 1029 2053 1012 7472 1029 2053 1012 5469 1029 2053 1012 2009 2134 1005 1056 2031 2505 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 2288 2023 4326 1010 4689 2671 2158 2007 15313 1011 2606 1010 1996 4438 2518 1012 2025 2613 2012 2035 1012 1998 1037 2158 2562 2893 3920 2035 1996 2051 1012 2009 3849 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] story chinese tall story tells the story of righteous monk trip ##ita ##ka , who , along with his guardians monkey , sandy and pigs ##y make their journey west on a quest to recover ancient sutra ##s , finally , they reach the final leg of their journey in sha ##che city but all is not as it seems when the city is attacked by evil tree demons . monkey tries his best to battle them but is overwhelmed , knowing his master is in grave danger , he uses his trust ##y golden staff to thrust trip ##ita ##ka to safety . < br / > < br / > the monk ends up being knocked out when he land and when he wakes [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2466 2822 4206 2466 4136 1996 2466 1997 19556 8284 4440 6590 2912 1010 2040 1010 2247 2007 2010 14240 10608 1010 7525 1998 14695 2100 2191 2037 4990 2225 2006 1037 8795 2000 8980 3418 26567 2015 1010 2633 1010 2027 3362 1996 2345 4190 1997 2037 4990 1999 21146 5403 2103 2021 2035 2003 2025 2004 2009 3849 2043 1996 2103 2003 4457 2011 4763 3392 7942 1012 10608 5363 2010 2190 2000 2645 2068 2021 2003 13394 1010 4209 2010 3040 2003 1999 6542 5473 1010 2002 3594 2010 3404 2100 3585 3095 2000 7400 4440 6590 2912 2000 3808 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 8284 4515 2039 2108 6573 2041 2043 2002 2455 1998 2043 2002 17507 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 7613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 7613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] our deeds are the reason of this # earthquake may allah forgive us all [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] our deeds are the reason of this # earthquake may allah forgive us all [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2256 15616 2024 1996 3114 1997 2023 1001 8372 2089 16455 9641 2149 2035 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2256 15616 2024 1996 3114 1997 2023 1001 8372 2089 16455 9641 2149 2035 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] forest fire near la ron ##ge sas ##k . canada [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] forest fire near la ron ##ge sas ##k . canada [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3224 2543 2379 2474 6902 3351 21871 2243 1012 2710 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3224 2543 2379 2474 6902 3351 21871 2243 1012 2710 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] all residents asked to ' shelter in place ' are being notified by officers . no other evacuation or shelter in place orders are expected [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] all residents asked to ' shelter in place ' are being notified by officers . no other evacuation or shelter in place orders are expected [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2035 3901 2356 2000 1005 7713 1999 2173 1005 2024 2108 19488 2011 3738 1012 2053 2060 13982 2030 7713 1999 2173 4449 2024 3517 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2035 3901 2356 2000 1005 7713 1999 2173 1005 2024 2108 19488 2011 3738 1012 2053 2060 13982 2030 7713 1999 2173 4449 2024 3517 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 13 , 000 people receive # wild ##fires evacuation orders in california [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 13 , 000 people receive # wild ##fires evacuation orders in california [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2410 1010 2199 2111 4374 1001 3748 26332 13982 4449 1999 2662 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2410 1010 2199 2111 4374 1001 3748 26332 13982 4449 1999 2662 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] just got sent this photo from ruby # alaska as smoke from # wild ##fires pour ##s into a school [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] just got sent this photo from ruby # alaska as smoke from # wild ##fires pour ##s into a school [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2074 2288 2741 2023 6302 2013 10090 1001 7397 2004 5610 2013 1001 3748 26332 10364 2015 2046 1037 2082 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2074 2288 2741 2023 6302 2013 10090 1001 7397 2004 5610 2013 1001 3748 26332 10364 2015 2046 1037 2082 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bert.run_classifier.InputFeatures at 0x7f963044dac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c4455f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044dd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044df98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044dbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044de80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c445550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044dd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c4454e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044dc18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044ddd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c445748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963044da90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c4459e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b74a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b75c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b74e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b78d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b76a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b76d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b70f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b79e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b75f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b77b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964a2b7fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304779b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304775c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304773c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304772e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304777b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304772b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304770b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304776a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304779e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304774a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304771d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304777f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304778d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304775f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304774e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304776d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304770f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630477320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047be10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bb00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bcf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047be80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bdd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047ba20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bc18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bf98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bf28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b5c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047ba58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bb70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047beb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047be48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bf60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047ba90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bb38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047bcc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963047b160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304814e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304815c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304810f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304811d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304819e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304816d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304817f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304819b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304810b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304814a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304813c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304816a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304812b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304815f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304817b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304818d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630481080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304812e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3244a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3246d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3245f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3243c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3240b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3249e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3242b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3240f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3241d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3247b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3246a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3248d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3249b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3245c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3244e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3242e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3247f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c324198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3297b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3292e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3293c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3299b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3296d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3294a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3291d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3298d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3296a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3295f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3297f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3299e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3294e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3295c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3290b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3290f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3292b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c329ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecb70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecb38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abeca58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec5c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecb00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abeccf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abeca20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecf98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abece80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abeca90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecf28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecdd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecf60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abec898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecc18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abece48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abeceb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abece10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abecd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f963abeccc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304737f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304736a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304738d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304739e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304732b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304734a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304734e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304735f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304737b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304730b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304739b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304731d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304730f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304732e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304736d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304735c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f96304733c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f9630473470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3325f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3324e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3321d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3327f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3320f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3326a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3322e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3327b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3325c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3324a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3329b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3328d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3323c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3329e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3326d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3320b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3322b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c332cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3372e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3376d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3370b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3379e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3372b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3375c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3374e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3371d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3374a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3379b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3373c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3370f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3377f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3376a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3375f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3378d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c3377b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964c337400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba820b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba828d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba820f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba827b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba824e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba824a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba826d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba825c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba826a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba822b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba822e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba825f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba823c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba827f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba829b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba821d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba829e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba82ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba886a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba888d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba880b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba889b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba884a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba882b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba883c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba889e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba881d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba882e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba887f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba886d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba880f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba885c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba887b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba885f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba884e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba88048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dc18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d6d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d0b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8de80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8df28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8de48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8db00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dfd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d9b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8deb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d8d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8db70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dc50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d7b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d4a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d1d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d5c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dbe0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d4e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dcf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8db38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8df98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d0f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8de10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8def0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8df60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8da20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8da90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dc88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d5f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8da58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dd68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dcc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dd30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d9e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8ddd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d2e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8dda0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d3c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d2b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba8d6a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93128>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93390>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba934a8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93978>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba932b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93b70>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93780>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93ac8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93588>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93550>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93d30>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba935c0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93f60>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93e10>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93198>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba937f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93940>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba935f8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba938d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93a90>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93a58>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba939e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93860>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93748>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93898>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93668>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93e48>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba932e8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93240>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93c18>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba939b0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba931d0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93160>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93f28>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93d68>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93a20>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93400>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93fd0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93ba8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba934e0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93b00>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93e80>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93828>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba936d8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93438>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93710>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93cc0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba930f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93cf8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba930b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93b38>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93630>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93518>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba936a0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93080>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93208>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93da0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93ef0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba933c8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93f98>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93048>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93c50>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93be0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93eb8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93dd8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba937b8>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93c88>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba93908>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba9a278>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba9a470>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba9a358>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba9a7f0>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba9a320>,\n",
       " <bert.run_classifier.InputFeatures at 0x7f964ba9a1d0>,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "q_WebpS1X97v",
    "outputId": "1648932a-7391-49d3-8af7-52d514e226e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9ba83faa90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9ba83faa90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "5d728e72-4631-42bf-c48d-3f51d4b968ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into output/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into output/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7012638, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7012638, step = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.562342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.562342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6556275, step = 100 (177.829 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6556275, step = 100 (177.829 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.628174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.628174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.2918747, step = 200 (159.191 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.2918747, step = 200 (159.191 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.629921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.629921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.19079992, step = 300 (158.750 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.19079992, step = 300 (158.750 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.6311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.1011751, step = 400 (158.453 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.1011751, step = 400 (158.453 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into output/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into output/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.620387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.620387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.21256024, step = 500 (161.190 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.21256024, step = 500 (161.190 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.635461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.635461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.048416924, step = 600 (157.366 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.048416924, step = 600 (157.366 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.630562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.630562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.078066185, step = 700 (158.589 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.078066185, step = 700 (158.589 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 713 into output/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 713 into output/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.022944856.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.022944856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:20:04.157188\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-12T21:04:20Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-12-21:06:05\n",
      "INFO:tensorflow:Saving dict for global step 468: auc = 0.86659324, eval_accuracy = 0.8664, f1_score = 0.8659711, false_negatives = 375.0, false_positives = 293.0, global_step = 468, loss = 0.51870537, precision = 0.880457, recall = 0.8519542, true_negatives = 2174.0, true_positives = 2158.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.86659324,\n",
       " 'eval_accuracy': 0.8664,\n",
       " 'f1_score': 0.8659711,\n",
       " 'false_negatives': 375.0,\n",
       " 'false_positives': 293.0,\n",
       " 'global_step': 468,\n",
       " 'loss': 0.51870537,\n",
       " 'precision': 0.880457,\n",
       " 'recall': 0.8519542,\n",
       " 'true_negatives': 2174.0,\n",
       " 'true_positives': 2158.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [prediction['labels'] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"cool\",\n",
    "  \"th\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "0                    Just happened a terrible car crash\n",
      "1     Heard about #earthquake is different cities, s...\n",
      "2     there is a forest fire at spot pond, geese are...\n",
      "3              Apocalypse lighting. #Spokane #wildfires\n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan\n",
      "5                    We're shaking...It's an earthquake\n",
      "6     They'd probably still show more life than Arse...\n",
      "7                                     Hey! How are you?\n",
      "8                                      What a nice hat?\n",
      "9                                             Fuck off!\n",
      "10                                No I don't like cold!\n",
      "11                           NOOOOOOOOO! Don't do that!\n",
      "12                               No don't tell me that!\n",
      "13                                            What if?!\n",
      "14                                             Awesome!\n",
      "15    Birmingham Wholesale Market is ablaze BBC News...\n",
      "16    @sunkxssedharry will you wear shorts for race ...\n",
      "17    #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...\n",
      "18    Check these out: http://t.co/rOI2NSmEJJ http:/...\n",
      "19    PSA: IÛªm splitting my personalities.\\n\\n?? t...\n",
      "20         beware world ablaze sierra leone &amp; guap.\n",
      "21    Burning Man Ablaze! by Turban Diva http://t.co...\n",
      "22    Not a diss song. People will take 1 thing and ...\n",
      "23    Rape victim dies as she sets herself ablaze: A...\n",
      "24         SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC\n",
      "25    @CTVToronto the bins in front of the field by ...\n",
      "26    #nowplaying Alfons - Ablaze 2015 on Puls Radio...\n",
      "27    'Burning Rahm': Let's hope City Hall builds a ...\n",
      "28    @PhilippaEilhart @DhuBlath hurt but her eyes a...\n",
      "29    Accident cleared in #PaTurnpike on PATP EB bet...\n",
      "...                                                 ...\n",
      "3233  Wreckage 'Conclusively Confirmed' as From MH37...\n",
      "3234  #science Now that a piece of wreckage from fli...\n",
      "3235  Wreckage 'Conclusively Confirmed' as From MH37...\n",
      "3236  Wreckage 'Conclusively Confirmed' as From MH37...\n",
      "3237  TOP STORY: wreckage from #MH370 officially con...\n",
      "3238  Wreckage 'Conclusively Confirmed' as From MH37...\n",
      "3239  Wreckage 'Conclusively Confirmed' as From MH37...\n",
      "3240  RT @australian Debris found on an Indian Ocean...\n",
      "3241  Cramer: Iger's 3 words that wrecked Disney's s...\n",
      "3242  Almost *wrecked* my van the other day because ...\n",
      "3243  'What manner of human being would parcel out a...\n",
      "3244  @Nathan26_RFC thought you said Saturday night ...\n",
      "3245  I just wanna ease your mind and make you feel ...\n",
      "3246  @yakubOObs think he deactivated because his no...\n",
      "3247  RT CNBC '3 words from Disney CEO Bob Iger wrec...\n",
      "3248  Smackdown tyme this should put me in a good mo...\n",
      "3249  @thrillhho jsyk I haven't stopped thinking abt...\n",
      "3250  @stighefootball Begovic has been garbage. He g...\n",
      "3251                 Wrecked today got my hattrick ????\n",
      "3252  #Ebola #EbolaOutbreak Ebola Virus: Birmingham ...\n",
      "3253  Malaysian PM confirms debris is from missing f...\n",
      "3254  Officials: Alabama home quarantined over possi...\n",
      "3255  See the 16yr old PKK suicide bomber who detona...\n",
      "3256  To conference attendees! The blue line from th...\n",
      "3257  The death toll in a #IS-suicide car bombing on...\n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
      "3259  Storm in RI worse than last hurricane. My city...\n",
      "3260  Green Line derailment in Chicago http://t.co/U...\n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
      "3262  #CityofCalgary has activated its Municipal Eme...\n",
      "\n",
      "[3263 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['happened terrible car crash',\n",
       " 'heard earthquake different cities stay safe everyone',\n",
       " 'forest fire spot pond geese fleeing across streetcannot save',\n",
       " 'apocalypse lighting spokane wildfires',\n",
       " 'typhoon soudelor kills china taiwan',\n",
       " 'shakingits earthquake',\n",
       " 'theyd probably still show life arsenal yesterday eh eh',\n",
       " 'hey',\n",
       " 'nice hat',\n",
       " 'fuck',\n",
       " 'nodont like cold',\n",
       " 'nooooooooo dont',\n",
       " 'dont tell',\n",
       " '',\n",
       " 'awesome',\n",
       " 'birmingham wholesale market ablaze bbc news fire breaks birminghams wholesale market',\n",
       " 'sunkxssedharry wear shorts race ablaze',\n",
       " 'previouslyondoyintv toke makinwas marriage crisis sets nigerian twitter ablaze',\n",
       " 'check',\n",
       " 'psa im splitting personalities techies follow ablazeco burners follow ablaze',\n",
       " 'beware world ablaze sierra leone amp guap',\n",
       " 'burning man ablaze turban diva via etsy',\n",
       " 'diss song people take thing run smh eye opener though set game ablaze cyhitheprynce',\n",
       " 'rape victim dies sets ablaze old girl died burn injuries set ablaze',\n",
       " 'setting ablaze',\n",
       " 'ctvtoronto bins front field house wer set ablaze day flames went rite hydro pole wonder',\n",
       " 'nowplaying alfons ablaze puls radio pulsradio',\n",
       " 'burning rahm lets hope city hall builds giant wooden mayoral effigy feet tall amp sets ablaze johnkass',\n",
       " 'philippaeilhart dhublath hurt eyes ablaze insulted anger',\n",
       " 'accident cleared paturnpike patp eb cranberry slow back traffic',\n",
       " 'got love burning self damn curling wandswear someone needs take away cuase im accident prone',\n",
       " 'hate badging shit accident',\n",
       " 'car recorder zeroedge duallens car camera vehicle trafficdriving historyaccident camcorder large',\n",
       " 'coincidence curse still unresolved secrets past accident',\n",
       " 'trafficsouthe roadpoleast accident near lewes kingston roundabout rather',\n",
       " 'sakumaen pretend feel certain way feeling become genuine accident hei darker black manga anime',\n",
       " 'legal medical referral service call us accident slipandfall dogbite',\n",
       " 'theres construction guy working disney store huge gauges ears bloody accident waiting happen',\n",
       " 'robynjilllian wlsdomteethsfeel like im going accident teesha gon na come',\n",
       " 'northbound junctions currently delays mins due accident',\n",
       " 'daveoshry soembie ifsay thatmet accident week would super jelly dave p',\n",
       " 'accident hit run cold block se vista ter gresham gresham police',\n",
       " 'happened accident butlike',\n",
       " 'please donate spread word training accident left polevaulter kira grnberg paraplegic',\n",
       " 'please like share new page indoor trampoline park aftershock opening fall',\n",
       " 'bxckylynch foi roh aftershock las vegas procura pirate bay que tem',\n",
       " 'schoolboy aftershock original mixexcision amp skism sexism far loud remixfirebeatz schella dear new',\n",
       " 'ir icemoon aftershock djicemoon dubstep trapmusic dnb edm dance ices',\n",
       " 'aftershock happened nepal last intl team still way responders chief collins lacofd',\n",
       " 'ir icemoon aftershock djicemoon dubstep trapmusic dnb edm dance ices',\n",
       " 'stop sayingwish start sayingwill unknown',\n",
       " 'want go aftershock october bandslisten nxt cant afford yet though gradschoolapps',\n",
       " 'still living aftershock hiroshima people still scars history edward bond',\n",
       " 'ir icemoon aftershock djicemoon dubstep trapmusic dnb edm dance ices',\n",
       " 'aftershock',\n",
       " 'victory bargain basement prices dwight david eisenhower',\n",
       " 'far best competitive maps imo hope invahnwetrust',\n",
       " 'brass copper cataclysm amp aftershock',\n",
       " 'jadeformkx happydont use aftershock variation counters play style hard',\n",
       " 'aftershock',\n",
       " 'carelessness leads aviation accident victim right seek compensation damages',\n",
       " 'rewindmusic found guys todayregarding accident airplane lol amp became fan sending love amp support cali',\n",
       " 'girl died airplane accident fifteen years ago',\n",
       " 'mintechan hihow keio line stationright btw know airplane accident near chofu airport week',\n",
       " 'mexican airplane accident ocampo coahuila mx killed coahuila government subsecretariat francisco garcia castells age',\n",
       " 'horrible accident man died wings airplane',\n",
       " 'god accident happen airplane idc rest luggage completely destroyed please save makeup',\n",
       " 'horrible accident man died wings airplane',\n",
       " 'update picture penn twp airplane accident',\n",
       " 'see judge ruled accident jfk airport involving korean air',\n",
       " 'thugiaurenhad airplane mode accident',\n",
       " 'years ago today baseball lost one greats airplane accident rip captain yankees yankees',\n",
       " 'experts france begin examining airplane debris found reunion island french air accident experts wedn',\n",
       " 'breakingnews experts france begin examining airplane debris found reunion island french air accident',\n",
       " 'experts france begin examining airplane debris found reunion island french air accident experts wedn',\n",
       " 'twelve feared killed pakistani air ambulance helicopter crash',\n",
       " 'im dying send ambulance mbelly brightasastar',\n",
       " 'ambulance sprinter automatic frontline vehicle choice lez compliant ebay',\n",
       " 'disneyxrowbrina ambulance isnt even parked well like nearly top someones car im laughing',\n",
       " 'shot times found dead cuffs involved car accident officers told ambulance treat',\n",
       " 'margaretcho call fag im going call ambulance rainbowpower',\n",
       " 'new nanotech device able target destroy blood clots science',\n",
       " 'want send medic supplies dads ambulance',\n",
       " 'held heroin ambulance',\n",
       " 'twelve feared killed pakistani air ambulance helicopter crash',\n",
       " 'helicopter ambulance ride transfer hospital miles away cost',\n",
       " 'privileged proud wear uniform nhs ambulance gayuk uniform proud privileged whatsyouremergency',\n",
       " 'bartender work described drunk man annihilated hsnowberger days',\n",
       " 'cop pulls drunk driver safety seconds car hit train via viralspell',\n",
       " 'cop pulls drunk driver safety seconds car hit train via viralspell',\n",
       " 'rosiegray sincerety think un would move israel fraction chance annihilated',\n",
       " 'johnboywest stage drama queen stand drama stage amp certainly never world stage would annihilated',\n",
       " 'drodrolagi handplacementgoals bro annihilated hashtag week',\n",
       " 'dostep outside seconds get annihilated mosquitoes',\n",
       " 'jacksfilms yiayplan well first strike dreamworks minions annihilated',\n",
       " 'nature appropriates love burn become annihilated beloved',\n",
       " 'ninahoag shred psych work friendship would annihilated',\n",
       " 'thehill example ofthe conservatives annihilated burtonwiimington prkng auth liberals stood amphave done nothing',\n",
       " 'aug kaiserjaegers wiped francis josephs crack regiment annihilated carso plateau',\n",
       " 'die everything annihilated',\n",
       " 'brooo got annihilated',\n",
       " 'albertbreer probably annihilated needed dd',\n",
       " 'gmcr longe rgreen mountain red mountainstock annihilated hours',\n",
       " 'please sign amp rt save saltriverwildhorses',\n",
       " 'allied plans annihilation german people louis nizer interesting',\n",
       " 'please share sign petition save wild horses arizona',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'new souls punished annihilation',\n",
       " 'latest updates salt river wild horse roundup via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'bosteenauthfest silent planet print annihilation audio',\n",
       " 'rt sign urgent stop annihilation salt river wild horses savewildhorses saltriverhorses',\n",
       " 'jackienatalydltdoonly get iced annihilation',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'incindia sonia gandhi like pakistan every defeat treated like victorytotal annihilation answer',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'stop annihilation salt river wild horses via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'spherulitic almost feels like si signs apocalypse lol',\n",
       " 'liked youtube video teamtwiistz futuristic house built survive apocalypse minecraft',\n",
       " 'liked youtube video minecraft night lucky block mod bob apocalypse wither amp mod showcase',\n",
       " 'one major reasons live big city rrs miss meals ever comes pass cue apocalypse script',\n",
       " 'askconnor theres zombie apocalypse item right weapon youre either screwed youre gon na live',\n",
       " 'apocalypse',\n",
       " 'itskingriffyall im excited apocalypse really dig xmen franchise goinglike event theme lot',\n",
       " 'tmfkco sounds like terrible time ill right',\n",
       " 'apocalypse artists go fukushima exclusion zone nuclear ura',\n",
       " 'southern prepper fictionsharecropping apocalypse apocalyptic doomsday prepper book reading',\n",
       " 'honestly could say apocalypse coming andwould exited hes enthusiastic everything',\n",
       " 'also nephew proof fat babies going save us apocalypse',\n",
       " 'liked youtube video zombie apocalypse rescue',\n",
       " 'added video youtube playlist futuristic house built survive apocalypse minecraft maps',\n",
       " 'live princes apocalypse dampd encounters meerkat',\n",
       " 'liked youtube video minecraft night lucky block mod bob apocalypse wither amp mod showcase',\n",
       " 'omg zombie apocalypse among students',\n",
       " 'liked youtube video minecraft night lucky block mod bob apocalypse wither amp mod showcase',\n",
       " 'universal order armageddon',\n",
       " 'patent pending stream armageddon ep',\n",
       " 'armageddon london',\n",
       " 'hitchbotheard might rise ashes armageddon go hitchbot like true superstar',\n",
       " 'turkey invades israel halfway armageddon',\n",
       " 'remember crowd around baggage carousel like armageddon bags last remaining food items earth animals',\n",
       " 'never compromise even face armageddonwatchmen',\n",
       " 'mustread vladimir putin issues major warning late escape armageddon pcraigroberts us rus',\n",
       " 'da one mtvsummerstar videoveranomtv mtvhottest britney spears lana del rey',\n",
       " 'build army lions leader dog fight lions die like dog',\n",
       " 'one direction pick fan army directioners',\n",
       " 'shark army black date stainless steel quartz men sport watch full read ebay',\n",
       " 'beyonce pick fan army beyhive',\n",
       " 'seconds summer pick fan army',\n",
       " 'beyonce pick fan army beyhive',\n",
       " 'victorinox swiss army date womens rubber mop watch',\n",
       " 'hope salvation army least gave tax receipt',\n",
       " 'one direction pick fan army directioners',\n",
       " 'one direction pick fan army directioners',\n",
       " 'one direction pick fan army directioners',\n",
       " 'check data upper wabash reservoirs louisvilleusace site',\n",
       " 'answertherefore came pass sixteenth yeardid go forth head army',\n",
       " 'one direction pick fan army directioners',\n",
       " 'seconds summer pick fan army',\n",
       " 'cheesehead report arson charges filed jackson county house fire',\n",
       " 'arson suspect linked fires caught northern california',\n",
       " 'israeli police unable solve case duma arson attack',\n",
       " 'shock report muslims setting wildfires across american westarson jihad',\n",
       " 'pnutsmama strong times like arson happens push thru past felony stage',\n",
       " 'breakingnews mourning notices stabbing arson victims stir politics grief israel posters rome',\n",
       " 'mourning notices stabbing arson victims stir politics grief israel posters shira banki',\n",
       " 'rt anopus elliott probation two years pay fines',\n",
       " 'rt saginaw police sketches persons interest saginaw arson',\n",
       " 'fire destroyed charlton winery ruled arson',\n",
       " 'wehtwtvwlocal trial date set man charged arson burglary',\n",
       " 'man charged connection shawnee arson burglary olathe kan man court wednesday accused',\n",
       " 'steele police arrest possible arson investigation',\n",
       " 'vixuhn rip arson',\n",
       " 'arson suspect linked fires caught northern california',\n",
       " 'rt theadvocatemag owner chicagoarea gay bar admits arson scheme lgbt',\n",
       " 'lgbtq news owner chicagoarea gay bar admits arson scheme frank elliott pleaded via theadvocatemag',\n",
       " 'mourning notices stabbing arson victims stir politics grief israel posters shira banki',\n",
       " 'contra costa co authorities arrest suspected serial arsonist authorities believe',\n",
       " 'owner chicagoarea gay bar admits arson scheme frank elliott pleaded guilty hiring arsonist',\n",
       " 'localarsonist cloudygoldrush man shit',\n",
       " 'mo way says carry',\n",
       " 'arsonist sets nyc vegetarian restaurant fire police newyork',\n",
       " 'arsonist torches house getaway vehicle police say surrey leader richardgearl',\n",
       " 'safyuanalready threw away letter tho',\n",
       " 'hotel arsonist caused damage crying help arsonist jailed court hear',\n",
       " 'suspected serial arsonist east bay arrest nightbeattv kbcwtv veronicadlcruz',\n",
       " 'arsonist mc christ walk toronto columbus gospel',\n",
       " 'bitch dont get slapped',\n",
       " 'trapvodka new fave vine',\n",
       " 'suspected serial arsonist arrested norcal',\n",
       " 'surveillance video captures man removing american flag long beach home ktla',\n",
       " 'serial arsonist gets bail jail release',\n",
       " 'arson suspect linked fires caught northern california',\n",
       " 'delhi government provide free treatment acid attack victims private hospitals aapatwork',\n",
       " 'im going panic attack',\n",
       " 'end innovation reduce attack surface industryinsights mcgsecure',\n",
       " 'realtwanbrown yesterdayhad heat attack whats funny relationship snapchat',\n",
       " 'texass gratuitously cruel attack dying gayman',\n",
       " 'cooper super pooper hero dog saved drowning detected heart attack beforeknew',\n",
       " 'double ebony attack blackhaired blowjob',\n",
       " 'suspect latest us theatre attack psychological issues',\n",
       " 'people tilly confused cat overcomes horrible attack win hearts',\n",
       " 'love love love massive attack angel hd va youtube',\n",
       " 'sunnyclaribel many dpjhodges articles attack labour defend conservatives see',\n",
       " 'rt rtcom tory attack freedom information assault govt accountability liberty',\n",
       " 'weirdhad near panic attack amp wanted write walked bathroom work nearly cried dreadfully miss',\n",
       " 'porn blacks attack white married slut',\n",
       " 'cop injured gunfight militants attack udhampur police post suspected militants attacked police post',\n",
       " 'newcastle schoolgirl attacked seaton delaval park pack animals led former friend newcastle',\n",
       " 'aus need career best along steve smith even try competing sura virus attacked team australia brutally',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'tennessee man killed police attacked family hatchet',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'attacked robotlvl ive earned total free satoshis robotcoingame bitcoin freebitcoin',\n",
       " 'attitude problem result constantly belittled bashed attacked amp demoralized fuck',\n",
       " 'mustve forgot memo getting attacked resident receiving concussion funny',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'logitechg fire nation attacked',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'metro nashville police simply best thank protecting us call via aol',\n",
       " 'peaceful happy twitter experience avalanche idiocy',\n",
       " 'car receiving tube installation avalanche sound output hhw',\n",
       " 'fucking truewill decapitated throw head avalanche',\n",
       " 'one glance avalanche drops one look heartbeat stops oh walk moon get mecouldnt said better',\n",
       " 'photo repost gregskazphotography repostapp rainierarms avalanche ambi charging handle',\n",
       " 'dad told would never let get avalanche',\n",
       " 'imaginehad plenty talk maths teacher liverpool similar age music',\n",
       " 'rare vintage collectable windriver avalanche edt spray mendealsuk',\n",
       " 'plus performance chip fuel saver chevy silveradoavalancheblazertahoe',\n",
       " 'dorion sens looking consistency hoffman coloradoavalanche avalanche',\n",
       " 'avalanche need anyone carry bag',\n",
       " 'musician kalle mattson recreates classic album covers clever music video avalanche',\n",
       " 'tix frozen fury xvii los angeles kingsavalanche rowl mgm grand',\n",
       " 'fc maid agency presents musician kalle mattson recreates classic album covers clever music video',\n",
       " 'avalanche rear axle assembly',\n",
       " 'senators another year determine hoffmanscoloradoavs nhlavalanche',\n",
       " 'manotwits kolaveriboy avast avalanche andals',\n",
       " 'colorado avalanche anaheim ducks tickets preseason',\n",
       " 'listen wtm avalanche take deep breath',\n",
       " 'theres avalanche men lunatics livingontheedge',\n",
       " 'environment lone pine trees growing across country remembrance battle gallipoli years ago',\n",
       " 'jackisheafferhave battle',\n",
       " 'exoticengram therasputin raspy cool already figured ttks battle shit',\n",
       " 'liked youtube video pokmon omega ruby alpha sapphire wifi battle vs',\n",
       " 'long battle spurs matt bonner took care brian scalabrine knockout vine nba',\n",
       " 'liked youtube video screwattack knuckles punches death battle',\n",
       " 'lonepine remembered around australia descendants grow via gallipoli',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps breakingnews',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps via usatoday',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps via usatoday',\n",
       " 'unsurprisingstill dismaying consequences went securing pathogen access',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'latest fallout biolab safety lapses fedex longer transport anthrax amp select agent research specimens',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'fedex stops shipping potential bioterror pathogens fedex corp nyse fdx longer deliver packages',\n",
       " 'citizens education project acted years watchdog dugway proving grounds',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'fedex longer transport bioterror germs via usatoday',\n",
       " 'offbeat news fedex stopped shipping bioterror pathogens',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps newsdict news fedex',\n",
       " 'bioterrorism ebola rt stopislam tcot ccot makedclisten teaparty',\n",
       " 'largest bioterrorism attack us soil involved tacos atlasobscura',\n",
       " 'time hedge catastrophic risks climate change asteroid impacts bioterrorism',\n",
       " 'uscourt amp colluded take whtusagov auth hostageamp make look blk wbioterrorismampuse lglorg idis id still',\n",
       " 'bioterrorism guidelines medical public health management',\n",
       " 'bbcworld bioterrorism drones reach large populated erea',\n",
       " 'bishoyragheb fair bioterrorism way go mean bsc isnt irrelevant',\n",
       " 'debtassassin forget threat bioterrorism',\n",
       " 'hsts anthony kimery discusses bioterrorism syfy channels joe rogan questions everything pathogen virus',\n",
       " 'anthony kimery discusses bioterrorism syfy channels joe rogan questions everything',\n",
       " 'harvardu amp colluded take whtusagov auth hostageamp make look blk wbioterrorismampuse lglorg idis id still',\n",
       " 'uganda seen front line bioterrorism fight',\n",
       " 'ppl livinghiv charged aggravated assault bioterrorism thingslow risk transmission hivisnotacrime',\n",
       " 'studying anthrax bioterrorism carpediem',\n",
       " 'gacourts amp colluded take whtusagov auth hostageamp make look blk wbioterrorismampuse lglorg idis id still',\n",
       " 'bioterrorism guidelines medical public health management donald hen',\n",
       " 'fight bioterrorism sir',\n",
       " 'fight bioterrorism sir',\n",
       " 'prsnvns thats good hear mine could better butcan manage alright bioterrorism doesnt wait',\n",
       " 'mariashriver fultoninfocourt trip keep strangersblkampwhtfrom usg ssapress id forfeit due bioterrorism lookap',\n",
       " 'afrojazz ill add youdont even know talking',\n",
       " 'never got paid give fuckwe might well blaze another one',\n",
       " 'junesnowpaw yeah gim dat creamy white stuff',\n",
       " 'reaction determines',\n",
       " 'port coquitlam fire crews say electric towel warmer started four alarm blaze last week destroyed several businesses',\n",
       " 'liked youtube video ashens blaze atgames handheld mega drives ashens',\n",
       " 'craving slurpees',\n",
       " 'omgneed go like yesterday',\n",
       " 'ovoizic baddiemoneysign blaze',\n",
       " 'lithgow news homeless blaze near portland',\n",
       " 'blizzarddracosawlets eat blaze blazinsmasher',\n",
       " 'big buzzy john blaze jus kame home year bid lovethyfamily',\n",
       " 'cruisingblazing banging',\n",
       " 'bitches say im hotsay bitch im blazing',\n",
       " 'bryanvsbracey ustadium need blazing speedthink watt far impressive obj another quick little guy',\n",
       " 'checked blazing horse tattoo yelp',\n",
       " 'midnight songcry goes realityin realitywas lonelyeven erased blazing sun',\n",
       " 'le ever blazing',\n",
       " 'packed ready go meet kkarmstrongg portugal tomorrow bring blazing sunshine fun times bestie',\n",
       " 'old ashwo lady festac town delta bbm leaked pictures',\n",
       " 'enjoying shade tree sun blazing cool breeze west hollywood park',\n",
       " 'happening fdny rescue blazing thruand blasting air horn',\n",
       " 'night insomnia whole stop blazing shit real',\n",
       " 'satelite shot series wildfires northern california continue blazing least five fire compl',\n",
       " 'website hosting get blazing speeds professional management unlimited options nyndesigns',\n",
       " 'come join us tomorrowaugust transcendblazing trail diversified world marketing',\n",
       " 'old ashwo lady festac town delta bbm leaked pictures',\n",
       " 'blazingroselia california hot blazing even though wildfires going even big massive rocky fire',\n",
       " 'people understand cant bleeding tweet everyone timedoes head stop ignoring hardly ignoring',\n",
       " 'nepal house near tia bleeding intl airline companies dry',\n",
       " 'battlehorkwishcould watch gif lu account posted mundo bleeding jesus',\n",
       " 'leona lewis ever see video dinah singing bleeding love',\n",
       " 'tombrevoort bleeding cool read tom brevoort though may look pictures',\n",
       " 'cut open andkeep bleeding bleedinglove leonalewis acapella singer canadiansinger',\n",
       " 'ears bleeding bass',\n",
       " 'kriszellner triplemania promise villano bleeding del rio whooping trick myztiziziziziz',\n",
       " 'looks like reynolds montano coming need stop bleeding best performance far',\n",
       " 'imkanomate stabbing really affect get stab wounds bleeding skarlet pushed backwards',\n",
       " 'yeahhate catsjust tried pet cat hand bite marks scratch marks bleeding dogsarebetterthancats',\n",
       " 'aichmomanic wraps cloth wound still bleeding ill ok falls ground im going die',\n",
       " 'ears bleeding',\n",
       " 'dudeandpal bad eyes ears bleeding',\n",
       " 'wouldnt want quit even ifwas bleeding',\n",
       " 'idkimnotfunny quickly blew mom doesnt get popular',\n",
       " 'lmfao snap blew',\n",
       " 'blew socan get follow back',\n",
       " 'atlwtmgc damn blew',\n",
       " 'wow social media really blew shania twain concert',\n",
       " 'cieistea idk blew',\n",
       " 'im always one phone call away many people done blew chances',\n",
       " 'well tweet blew fast',\n",
       " 'well blew whilewas sleeping',\n",
       " 'funnychubbyguy poor white guy mom made pizza wanted maccheese blew school sad story',\n",
       " 'wow bro blew quick believe hes funny clever person times like',\n",
       " 'tokyodotcomdidnt either till blew mentions lol',\n",
       " 'fucking around google maps work andpulled boise blew minds yeah idaho isnt think',\n",
       " 'stadium blew',\n",
       " 'balloon thing rudewould popped balloon whoever blew ups face',\n",
       " 'thefierygrave yeah true wouldnt got exposed probably wouldnt went mlg blew',\n",
       " 'reason players ego cause cod champs thats esports blew players strictly look orgs',\n",
       " 'thda kicks antiblight loan effort memphis',\n",
       " 'westmdcountypa land bank targets first latrobe building property acquisition fight blight',\n",
       " 'lots talk susceptible shoot blight without trama',\n",
       " 'help fight northern corn leaf blight headline amp',\n",
       " 'colemcfadyean fuck cole',\n",
       " 'imagine old gods awakened start blight biomarkdarrah mikelaidlaw',\n",
       " 'stamford tightens blight law dilapitated barn sits halferect hickory road near int stamford topix',\n",
       " 'cleveland heights shaker heights fight blight via clevelanddotcom',\n",
       " 'keep great work councilman brian oneill',\n",
       " 'blight every day',\n",
       " 'michelgrabowy vegetablefarmer internet loserlike leave snide remarksme person blight upon face earth',\n",
       " 'ok important question inquisition fighting darkspawn one gets blight entire game',\n",
       " 'member post norwalk blight ordinance said success properties change hands',\n",
       " 'time ripe yet another banana blight bananas appeal slipping future bananas',\n",
       " 'red eye crossing oceanpeeling potatoes servile devotionblight',\n",
       " 'detroit made progress blight many burned shells houses remain',\n",
       " 'justinfenton per sun piece last yearthink opposite done urban blight monumentalized',\n",
       " 'im renewable energy windfarmsdo agree abbott horrible blight landscape',\n",
       " 'buying mop',\n",
       " 'lonewolffur slaps',\n",
       " 'literally lunchhad cheese curds dinnerhad blizzard pls pray health',\n",
       " 'ohsee tied sweet blizzard',\n",
       " 'check new ebook sale timzak lizard stuck blizzard rhyming kidsbook amazon',\n",
       " 'deck building year old daughter',\n",
       " 'eddietrunk blizzard ozz',\n",
       " 'gunnar optiks heroes storm siege gaming glasses onyxfire wow blizzard full',\n",
       " 'game drove rolo mini blizzard',\n",
       " 'horrible moment whenopen dryer looks like snowy blizzard cuzleft piece paper jeans pocket',\n",
       " 'blizzard another month sub thing gave next wow expo free would probaly knock sub count pretty fucking high',\n",
       " 'internet sending subliminal messages ice cream thereforeneed blizzard rn',\n",
       " 'whoa new legend fun',\n",
       " 'louistomlinson ifrelate billie jean ur gon na also enjoy blood dance floor buybloodonthedanceflooronitunes',\n",
       " 'xoxoxxxooo rutinaofficial nbchannibal hannibal asks seen blood',\n",
       " 'bring horizon blood stained giant art print panel poster',\n",
       " 'aint crip bloodim doin thing',\n",
       " 'bitch im monster good blood sucker',\n",
       " 'criess blood golden wook life',\n",
       " 'blood makes related loyalty makes family sirbeezalot',\n",
       " 'youngthug remind blood sosa bellydo homework',\n",
       " 'yankees reason im blood pressure medicine',\n",
       " 'cant gay men donate blood',\n",
       " 'new mashup bad bloodwhere arenow comes tomorrow make surevine shaylencarroll',\n",
       " 'bloods penny post philly octagonal cds black grid cancel',\n",
       " 'seeing blood surgery regular gp behind desk say dentists arent real doctors ok',\n",
       " 'sometimes blood aint thicker water sometimes family bring quicker strangers',\n",
       " 'name chizugender maleage hair redeyes pinkdere type pasokonblood type type',\n",
       " 'whole new zealand shouting bloody marvellous john campbell join radio nz radionz',\n",
       " 'whats bloody use bus app everything prediction one driving bus today',\n",
       " 'continues best guest iloveggletters weeks episode bloody outrageous',\n",
       " 'bloody hot room',\n",
       " 'im bloody excited see maisy martha',\n",
       " 'jeremyclarkson buy new one bloody ell',\n",
       " 'youve twitter since bloody hell veteran',\n",
       " 'awful refereeing ruined balance game guy bloody mess imfc',\n",
       " 'precious lunch bloody hell brain issit',\n",
       " 'swear eyes bloody red bitchfeel amazing',\n",
       " 'shtballplayrsdo anyway running mouth bloody game pree sure youre bigger baseball calm hell',\n",
       " 'bloody nurses waking wheneventually fall asleep lol hospital',\n",
       " 'halfpeachh caitlincavanahcant deal right omg remembers bloody mary',\n",
       " 'nightmare elm street getting remade via bdisgusting',\n",
       " 'bloody hell teen wolfhayden liam cute andthink stiless jeep dead huh pleeeaseloved mtvteenwolf',\n",
       " 'luckily im cause mexicans came cut grass early andwas sleepwoulda full blown attitude',\n",
       " 'bored shit dont nobody fuck wit busy yo shit get blown smh',\n",
       " 'yes know youre adultressand yes killary emailslike smoke blown ur azznsa',\n",
       " 'sassynreal agreed whole thing blown faces right wrong indifferent public relentless court public opinion',\n",
       " 'dont understand white iverson post malone hasnt blown fuck already',\n",
       " 'jaykpurdy shush im getting blown poster size',\n",
       " 'license picture blown absolutely terrifying',\n",
       " 'deadly suicide bombing hits mosque saudi arabia suicide bomber blown mosque',\n",
       " 'phone blown rn',\n",
       " 'dodgersnation due get blown least still winnable game',\n",
       " 'lock last remaining shred manhood accept youre full blown sissy fe',\n",
       " 'get blown delicious flavor blowvape strawberry cherry pop try today',\n",
       " 'run singles rain blown',\n",
       " 'damn lastwitchhunter blown twitter feed looks amazing',\n",
       " 'malikchaimaahope zayn gets blown drone attack whilst visiting family pakistan',\n",
       " 'coldwood blown away support even hung art',\n",
       " 'bmrow first thought blownup dvd im pretty sure post mortem years back',\n",
       " 'new ladies shoulder tote handbag faux leather hobo purse cross body bag womens full read',\n",
       " 'cute soft washed fabric camouflage outdoor shoulder crossbody bag pack purple camouflage',\n",
       " 'cross body bag purse zippers shoulder bag camouflage camo phone case camera case wallet wallet wallets',\n",
       " 'louis vuitton monogram sophie limited edition clutch cross body bag full read ebay',\n",
       " 'nikon mp digital slr camera body batteries carry bag charger',\n",
       " 'auth louis vuitton brown saumur cross body shoulder bag monogram full read',\n",
       " 'dracula vampire cross body small shoulder bag',\n",
       " 'new diesel willyt printed front cross body shoulder bag one size ksl',\n",
       " 'new ladies shoulder tote handbag women cross body bag faux leather fashion purse full',\n",
       " 'nikon mp digital slr camera body batteries carry bag charger',\n",
       " 'new ladies shoulder tote handbag faux leather hobo purse cross body bag womens',\n",
       " 'would love win suzes amazing filled beauty bag giveaway contents loccitane body shop amp lavera',\n",
       " 'pl parenthood employee laughs aborted baby body parts mixed together bag',\n",
       " 'new ladies shoulder tote handbag faux leather hobo purse cross body bag womens',\n",
       " 'photo bonesymcbones beartooth body bag',\n",
       " 'twotone crossbody smartphone bag shipped via dians daily deals twotone',\n",
       " 'genuine leather man bag messenger fit ipad mini tablet case cross body air jp full rea',\n",
       " 'meekmillthink time consulted kendricklamar cause drake done body bagging',\n",
       " 'people bagging rouseys body shit id love body like ridiculing probably dudes skinny jeans',\n",
       " 'bagging last spot body attack getin traintuesday lesmills',\n",
       " 'dear lord forgive body bagging meekmill twice amen drake',\n",
       " 'otooledalton first cant hit stick shit second im body bagging sight',\n",
       " 'drake really body bagging meek atm',\n",
       " 'remember halo interview abc wasnt time pamanian devil body bagging fools',\n",
       " 'jerseys artist wolfgangjoc pays homage late great biggie smalls body bagging hypnotize beat',\n",
       " 'drake kill meek mill startedbagging dead body',\n",
       " 'man start body bagging local jokal photographers',\n",
       " 'body bagging',\n",
       " 'drake body bagging meek',\n",
       " 'body bagging thatthink time bring bags',\n",
       " 'need stop paying attention drizzy body bagging meekmill worry bout happen sandrabland',\n",
       " 'hear north niggas body bagging never know got spitters south',\n",
       " 'eish even drake killing niggas eish game really watered singing nigga body bagging rappers smh',\n",
       " 'esmcampy used werewolf also idiotawas tea bagging body like minutes fighting someone else',\n",
       " 'body bags movie review via youtube',\n",
       " 'lenngonewin nah man need defenseless thugs cops get get put cool new body bags',\n",
       " 'complexmag asking body bags pushat',\n",
       " 'rt saudi coming home yemen body bags saudi kingdom ego youre loo',\n",
       " 'hot ladies handbag leather shoulder tote satchel messenger cross body bags green handbags',\n",
       " 'fairfax co investigating firefighter khalil abdulrasheed facebook post saying police shld put body bags',\n",
       " 'mistakes come back body bags leading vuca world armed forces raghu',\n",
       " 'womens stylish metal chain pearls solid hanbags shoulder cross body bags golden',\n",
       " 'womens flower printed shoulder handbags cross body metal chain satchel bags pink',\n",
       " 'one top rted tweets japan screenshot young person claiming know anniversary hiroshima bomb',\n",
       " 'hiroshima marks years since bomb japanese city hiroshima marks anniversary dropping',\n",
       " 'joannabomb eventchase mtv mtvnews last year atw theyre multi platinum artists',\n",
       " 'peytonlaynec ohhh coming agehad bomb teacher never anything final went salt grass bought us food',\n",
       " 'bestfriends bomb',\n",
       " 'rip rt ajenews aug tokyo time worlds atom bomb hiroshima',\n",
       " 'joshrogin jimsciutto natseccnn suspected kerry barry got taken ride iran making bomb talks went',\n",
       " 'jonathanshaininthink bomb raises sorts great questions isnt one',\n",
       " 'american censors supressed accounts suffering nagasaki',\n",
       " 'hometown hit hiroshima atomic bomb safemode',\n",
       " 'johncusack text dropped abomb',\n",
       " 'bruh sex beachis bomb fuck yo',\n",
       " 'hansel gretel witch hunters bomb ass movie yehkno im late',\n",
       " 'ltkyodonewsgt hiroshima marks abomb anniv amid fears eroded pacifism followme japan',\n",
       " 'mattielbreauxthe bomb',\n",
       " 'aunt gave drink game bomb ill never tell though shhhh',\n",
       " 'tell shinichi kudo im giving minutes enjoy moriya teiji famous architect time bombed skyscraper',\n",
       " 'photo bombed princess',\n",
       " 'get call back interview thought bombed gtgtgt',\n",
       " 'hour yrs ago one greatest acts mass murder world history occurred hiroshima bombed',\n",
       " 'years ago nowish america bombed hiroshima justified allcan say nomorehiroshimas fepow',\n",
       " 'hiroshima look like bombed city looks monster steamroller passed squashed existence',\n",
       " 'july hiroshima bombed nuclear weapon bomb named little boyampcarried plane called enola gay',\n",
       " 'allenpeacock deal cleariran either agrees deal abandons program gets bombedits deal right wing love',\n",
       " 'picture cute sleeping puppy photo bombed demon back',\n",
       " 'johnejefferson remind somaliaeritrea amp nigeria bombed uk asylum applicants uk fromamp e',\n",
       " 'sitctips vlog get videobombed',\n",
       " 'bombed fake bolt chat everyone thought bolt ended lmfao',\n",
       " 'bombing hiroshima',\n",
       " 'dont really talk murder bombing campaigns wwii murder',\n",
       " 'hiroshima marks anniversary atomic bombing via nhk world news',\n",
       " 'japan mark anniversary hiroshima atomic bombing',\n",
       " 'happened asia japan marks anniversary hiroshima atomic bombing',\n",
       " 'like survive atomic bombing hiroshima',\n",
       " 'anniversary hiroshima bombing',\n",
       " 'japan marks anniversary atomic bombing hiroshima',\n",
       " 'japan marks anniversary hiroshima atomic bombing associated press via nyt',\n",
       " 'sanitised narrative hiroshimas atomic bombing japan',\n",
       " 'japan marks anniversary hiroshima atomic bombing bells tolled hiroshima thursday japan marke',\n",
       " 'japan marks anniversary hiroshima atomic bombing',\n",
       " 'yrs since atomic bombing hiroshima terrible mass murder',\n",
       " 'breaking news australia collapse hapless trent bridge',\n",
       " 'project syndicate marshall plan united states',\n",
       " 'new two giant cranes holding bridge collapse nearby homes',\n",
       " 'akinwunmiambode urgentthere currently storey building church bstop oworoshoki third mainland bridge likely collapse',\n",
       " 'throwback thursday thoughts citizen media minneapolis bridge collapse disaster tbt',\n",
       " 'ashes australias collapse trent bridge among worst history england bundled australia',\n",
       " 'two giant cranes holding bridge collapse nearby homes',\n",
       " 'throwback thursday minneapolis bridge collapse amp citizen journalism tbt',\n",
       " 'berggrueninst berggruen institute member dambisamoyo problems us infrastructure prosyn',\n",
       " 'australia collapse trent bridge twitter first morning fourth ashes test',\n",
       " 'shane warne ricky ponting shocked australia batting collapse trent bridge',\n",
       " 'injured missing bridge collapse central mexico',\n",
       " 'australias ashes disaster collapse unfolded trent bridge telegraph',\n",
       " 'sport shane warne ricky ponting shocked australia batting collapse trent bridge australian grea',\n",
       " 'injured bridge collapse swiss kosher hotel accident occurred tuesday metropol hotel',\n",
       " 'scary thought happened today would teenagers taking selfies front burning buildings',\n",
       " 'sounds like song would hear movie walking away burning buildings cars shit',\n",
       " 'ctvkathyle news dont run burning buildings',\n",
       " 'forestservice rt dhsscitech firefighters run burning buildingswe work tech',\n",
       " 'like folks said burning buildings doesnt workonlyit cities trying get information asap prevent',\n",
       " 'like ppl burning buildings knowing originally created',\n",
       " 'kou like cash register buildings burning',\n",
       " 'thehill hate crimes looney libs always burning buildings vandalzing private property',\n",
       " 'rules long life dont go burning buildings dont jump perfectly good airplanes dont awaken sleeping giants',\n",
       " 'port coquitlam fire burning several businesses',\n",
       " 'speechgirlam par caring sick helping people move talking movies rescuing people burning buildings',\n",
       " 'travdave kornbreadicu especially happened really nothing tear burning buildings saves city money real',\n",
       " 'fire brigade play guys music burning buildings put flames',\n",
       " 'two buildings burn antioch fire two buildings burning fire delta pines',\n",
       " 'denisleary sure folks rush burning buildings im grateful trueheroes',\n",
       " 'jane kelsey fire economy aug old govt buildings wgtonthe context amp driver tpp tradeinservicesagreement',\n",
       " 'failure doesnt exist change direction alejandro jodorowsky via buildingsonfire',\n",
       " 'im security want help case emergency like buildings fire shooters building im leaving tho',\n",
       " 'blainescronuts thats whatwould buildings fire',\n",
       " 'fucking apartments across street fire right fuckin firetrucks wtf second time buildings caught fire',\n",
       " 'testimony human spirit us dont set fire office buildings sunday night',\n",
       " 'multiple buildings fire downtown hinton okthe fire threatening level entire block',\n",
       " 'dnrres buildings shelled amp fire gorlovka tonightcivilian casualties janjuly killed incl children wounded',\n",
       " 'gilsimmons lightening struck amp units buildings development fire groton ct check fb wall recent post',\n",
       " 'taizhouthi saleh indiscriminate shelling castle sets tahreer st buildings fireyemen',\n",
       " 'buildings fire behind tisas niceville tristapnwfdn',\n",
       " 'two buildings involved fire street manchester wmur',\n",
       " 'agricultural buildings fire aldwark nyorks fire amp rescue service still grim news fewer piglets caught',\n",
       " 'multiple firefighters hospitalized fighting fire manchester nh',\n",
       " 'multiple buildings fire downtown hinton okthe fire threatening level entire block',\n",
       " 'happy pensioner storiessafe buildings watching report unsafe cladding catching fire melbourne',\n",
       " 'multiple housesbuildings fire kalispell',\n",
       " 'family heartbrokenmobile home nellis amp cheyenne burned fire kids ready go back school',\n",
       " 'burned fingers',\n",
       " 'burned stake',\n",
       " 'let tell brandace brought today ntc heat allwe survived burned',\n",
       " 'irsolichin potus last cities everyone burned death terrorism grandest scales',\n",
       " 'kyraelizabethh back break light burned sogotta show next days',\n",
       " 'realdonaldtrump burned republicans many times least talking things important us good luck',\n",
       " 'burned calories minutes total body cardio myfitnesspal',\n",
       " 'corey white talking dat shit got burned curlin irons black women necks',\n",
       " 'blackhatwriter yeah im going home computer next two weekends getting really burned really quick',\n",
       " 'pdleague im sorry crit sure matter prob cuzwas burned',\n",
       " 'got rhymes bibles got psalmsand like prodigal son ive returnedanyone stepping youll get burned',\n",
       " 'widouglass kurrrlyw adamrubinespn nationals welp dunce got burned',\n",
       " 'omg last weekaccidentally burned cigarette lol onlywould',\n",
       " 'adamrubinespnknow burned week ago similar spot getting outjust pitches save inn',\n",
       " 'keep calling crazycausenever learnedyou stop loving firebecause got burned',\n",
       " 'im proud heart played stabbed cheated burned broken somehow still works',\n",
       " 'hermancranston wired fires burning firefighters budgets',\n",
       " 'psachopped jalapenos amp fingers burninglavaonfire painful hrs first wear gloves peppers hell',\n",
       " 'geoengineering burning fossil fuels making global weather unstable temps biggest flood past years',\n",
       " 'sargiuliani im currently burning skin',\n",
       " 'yes comrades sweatseeking calorieburning cardiotorture need support get smile nod wave',\n",
       " 'wow legion slouching towards broken isles warlords draenor wasnt close enough burning crusad',\n",
       " 'thevenskus led killer workout today thesweatshoppe core burning gymtime worktime auditiontime roccotime nobs',\n",
       " 'peddle floorboard end four door burning backroad song park pile baby watch step',\n",
       " 'burning bridges forte',\n",
       " 'rt real burn book entry ca fuck place always burning',\n",
       " 'fear fever feel nowfeel fire burning beloooow gon na trick ya swallow whooooole mtvhottest justin bieber',\n",
       " 'burning legion returned',\n",
       " 'arab autocracies burning house via theeconomist',\n",
       " 'tarekfatah burning enemity pakistanm sure burn',\n",
       " 'put ton sunscreen im still burning',\n",
       " 'blizzard details new dungeons raids world warcraft legion burning legion',\n",
       " 'accomplishing burning killing buildings',\n",
       " 'amp common breaker trip threshold protect awg wire commonly used buildings burning power outlets results',\n",
       " 'burning buildingskeep flames lit',\n",
       " 'threealarm fire burning antioch apartment complex',\n",
       " 'rockbottomradfm one challenges tough enough rescuing people burning buildings',\n",
       " 'mrjamesob ure getting rid every last drop life gonecrawl burning buildings life matters jakey',\n",
       " 'firefighters brave people opinion bcclaredont knowcould save ppl burning buildings housessalute',\n",
       " 'rocky fire burning ca still threatening buildings largest state amp one nearly dozen burning',\n",
       " 'liked youtube video mega man zx ost ogre claw area burning buildings',\n",
       " 'denisleary sure folks rush burning buildings im grateful trueheroes',\n",
       " 'trubeque destruction magics fine dont go burning buildings',\n",
       " 'high skies burning buildings nowplaying',\n",
       " 'used drones help firefighters lead people burning buildings help put fire',\n",
       " 'attack pp attack womens health amp attack womens health attack americas health',\n",
       " 'soaring temperatures southern us states causing bush fires pity scotgov allow rain scotland tweetlikekez',\n",
       " 'know hot bush fires right near villa',\n",
       " 'bushfire causes first victim albania multiple fires albania caused first vict bush',\n",
       " 'drove past field fire yesterday reminded bush fires used get south africa',\n",
       " 'bush food fires',\n",
       " 'watch mother nature halts australia blue mountains bushfire',\n",
       " 'jiminswangwas say bush fires aus reason hot',\n",
       " 'bushfire causes first victim albania multiple fires albania caused first vict bush',\n",
       " 'like goats dislike bush fires probably video',\n",
       " 'pamelamueller time honour privlege prayhave witnessed bush fires living australia',\n",
       " 'therealpbarry much erupting volcano bush fires raging put atmosphere none believers mention',\n",
       " 'eugene fires got ta make think hashtag conspiracies hashtag population control hashtag bush',\n",
       " 'warfighting robots could reduce civilian casualties calling ban premature',\n",
       " 'lets hope concert ends zero casualties amen',\n",
       " 'american weapons support fueling bloody air war yemen',\n",
       " 'women children casualties afghan violence',\n",
       " 'gtiso fair enough change always unfortunate casualtieslike nzmorningreport utilises welly amp aucklnd well see',\n",
       " 'memorial day years hiroshima nagasaki august wth total casualties god forgive',\n",
       " 'warfighting robots could reduce civilian casualties calling ban premature',\n",
       " 'esksf always casualties right thing especially going cost boss money',\n",
       " 'joshdaniel mikegiarrusso foxnews think bunch untrained people firing back isnt going cause casualties',\n",
       " 'obama lambasted respected amp jokewould trump prefer civilian casualtiesgopmaddow',\n",
       " 'jimwinnm kurtschlichter heres thing predicting million american casualties forget goddamn japanese',\n",
       " 'american weapons support fueling bloody air war yemen',\n",
       " 'lets appreciate group people everyone hates much police prevented mass casualties today nashville',\n",
       " 'dontestallworth stevesgoddard look casualties iwo jima okinawa numbers dead sides youll see',\n",
       " 'cameronsurname allies forecast invasion japan minimum million casualties minimum',\n",
       " 'rxcasualty vpzedd dainttm call',\n",
       " 'legaljobs litigation attorney scottsdale az claims specialist ii casualty bodily injury jobs',\n",
       " 'social casualty',\n",
       " 'maccocktail first casualty war truth hiram johnson died day tony blairs case died war',\n",
       " 'georgian wines become casualty russias political campaign ukraine',\n",
       " 'bruminthecity oh coffee espresso poss casualty todays busiest thurs ever come perfect coffee house',\n",
       " 'associated agencies inc hiring property amp casualty account manager jobs rollingmeadows insurance',\n",
       " 'documenting climate changes first major casualty via greenharvard',\n",
       " 'canceling deal warships france agrees repay russia via nytimes',\n",
       " 'charlie casualty ashes',\n",
       " 'keep alive sound intensity level catering keeping auditory effect casualty insurance hcxvneoav',\n",
       " 'give money hear social casualty live mtvhottest buck id give',\n",
       " 'vpzedd reach skype',\n",
       " 'please read casualty fanslet us know amp help spread word retweeting dm scriptettesar info',\n",
       " 'business casualty',\n",
       " 'datshemmings moi cest plutot disconnected reason close strangers heartbreak girl rejects social casualty ufhguhfidt',\n",
       " 'fox news biggest media catastrophe american history people love living lie',\n",
       " 'twist sony ultimate preparedness library prepare catastrophe',\n",
       " 'buzzfeed stannis evil books got catastrophe saying',\n",
       " 'shaneryanhere catastrophe amazon prime',\n",
       " 'youa wonderful person thomasistrash thomasistrash',\n",
       " 'nowam quietly waiting catastrophe personality seem beautiful interesting modern',\n",
       " 'gaza summer catastrophe gaza palestine israel bds',\n",
       " 'caradelevingne mother chucker catastrophe much hotness one room cant cope swifties',\n",
       " 'twlldun peterjukes good piece andagree failure postwar planning catastrophe',\n",
       " 'sorry kylie pretty sure james ate bruce morning catastrophe',\n",
       " 'robdelaney desperate watch last episode catastrophe canget uk',\n",
       " 'barackobama hello mr president really big problem puerto rico regarding water situation like catastrophe',\n",
       " 'alaskas wolves face catastrophe alexander archipelagowolves population plummeted yr voice',\n",
       " 'two shows ive loved summer catastrophe unreal',\n",
       " 'vaunwilmott paystara agrees catastrophic loss love barn scene short shirtless michael dominion',\n",
       " 'something catastrophic coming tune',\n",
       " 'wp following catastrophic event united states decimated president prepares first post apocaly',\n",
       " 'johngreen catastrophic history',\n",
       " 'rise wages going catastrophic impact wage structure fuck automation going',\n",
       " 'unitednyblogs insidecityhall nyclass ouch needs explain nyclass hate mailer catastrophic fail',\n",
       " 'catastrophic effects hiroshima nagasaki atomic bombings still felt today',\n",
       " 'society respond looming crisis uncertain timing catastrophic proportions',\n",
       " 'dear carlyfiorina anyone family alive climatechange gopdebate feelthebern',\n",
       " 'cancer everything seems catastrophic feel worse hit rock bottom recover start got',\n",
       " 'jim rickards blog catastrophic outcomes may come faster expecte',\n",
       " 'stop extreme regulations hurt middle class',\n",
       " 'laladeviluke consequences could catastrophicwas also informed security ai mute primarily',\n",
       " 'gaming learning legacy catastrophic eruption fifteenmilehigh plume ash eruption wh',\n",
       " 'want sell boat car truck',\n",
       " 'long baby blissfully unaware catastrophic day happy fed bathed cared',\n",
       " 'suffered catastrophic pants failure work todayresorted tying jacket around waist tilgot thrift store trustory',\n",
       " 'society collapse due catastrophic food shortages says study',\n",
       " 'dear sentedcruz anyone family alive climatechange gopdebate',\n",
       " 'something catastrophic coming tune via post',\n",
       " 'chemical brothers play armory sf tomorrow night emergency bay area edm announcement chem',\n",
       " 'fake explosion toxic chemical drills emergency responders pope francis million visit sept',\n",
       " 'emergency crews scene chemical spill spohn shoreline details come us',\n",
       " 'standard behavior chemical mixers emergency purposesjyb',\n",
       " 'emergency services called bacup strong chemical smells',\n",
       " 'rossendale emergency services called bacup strong chemical smells rossendale',\n",
       " 'emergency services hammondville near jewell close story liverpool',\n",
       " 'chemical spill house hammondville emergency services attending breakingnews',\n",
       " 'bendwavy emergency chemical rinse',\n",
       " 'emergency services called bacup strong chemical smells',\n",
       " 'pmbreedlove russian nuclearbiologicalchemical nbc brigade emergency response exercise southern md',\n",
       " 'emergency prompted due chemical spill factory near redruthuk',\n",
       " 'god pushes edge cliff two things may happen catch fall teach fly',\n",
       " 'photographer brian ruebs endures climb capture bride groom rarasathie',\n",
       " 'might go losing itdrive cliff fall void',\n",
       " 'hope fall cliff',\n",
       " 'spencervhhope fall cliff',\n",
       " 'usetheiight youre ugly piece trashhopefall cliff',\n",
       " 'huge cliff landslide road china watch moment cliff collapses huge chunks rock fall onto',\n",
       " 'boy falloffcliff still got man rumor going back de',\n",
       " 'video man rescued cliff fall shaldonread',\n",
       " 'mattbuek pitchers gracefully lose stuff fall cliff land throwing arm hard tell gets',\n",
       " 'johnnieguilbert jump cliff ill cheering back flip fucks sakehope fall break neck',\n",
       " 'people im camping withwould happily see fall cliff',\n",
       " 'mega keen hiking tomorrow hopedont fall cliff haha keen fitness yay excitedmuch',\n",
       " 'ntsouthwest looks tasty problem cliff might fall sea due coastal erosion would crumbly',\n",
       " 'interview collapse materialism best talkradio listen live',\n",
       " 'greeces tax revenues collapse debt crisis continues',\n",
       " 'correction tent collapse story hnlnow',\n",
       " 'correction tent collapse story',\n",
       " 'knew cain wasnt going die either val someone gon na collapse later emmerdale summerfate',\n",
       " 'interview collapse materialism best talkradio listen live',\n",
       " 'west side water main break causes street collapse video el paso times',\n",
       " 'fouseytube zaynmalikwould collapse',\n",
       " 'guardian greeces tax revenues collapse debt crisis continues guardian fresh',\n",
       " 'oikawa karasunoguess id collapse overwhelming happiness',\n",
       " 'residents set sign tribute families lost lives thane building collapse unfortun',\n",
       " 'technical collapse gt wlb new low',\n",
       " 'arnejungjohann energiewende directly responsible collapse energy value',\n",
       " 'jfb tfb correction tent collapse story smantibatam',\n",
       " 'ashes rampant england humiliate rival australia',\n",
       " 'greeces tax revenues collapse debt crisis continues talks continue proposed third bailout',\n",
       " 'ppact hasnt already collapsed testament thedemocrats lockstep devotion politics reality',\n",
       " 'sheriffclarke carpenter built house collapsed would ask carpenter rebuild',\n",
       " 'memorial honoring died bridge collapsed',\n",
       " 'used good time june july summer collapsed smh',\n",
       " 'remember whenwas little anddidnt want say pledge school collapsed pit hell opened',\n",
       " 'ceiling walls collapsed upon darknesswas trapped',\n",
       " 'collapsed bathroom bcuz michael',\n",
       " 'video link billowing fire southtowne lanes roof collapsed',\n",
       " 'lashak quinlo lsedig yemmely inmianajonessaw crazy edit twisted mixed tut andcollapsed',\n",
       " 'view storm developed amp collapsed albertville evening wind damage reported valleywx',\n",
       " 'michaelgbaron panic city collapsed party city full swing',\n",
       " 'jessicalauraxo grandma collapsed died',\n",
       " 'inquiry launched collapsed hongkong furniture retailer dsc scmpnews',\n",
       " 'official crowd tent collapsed told stay put lancaster nh circus workers first told spectators',\n",
       " 'jemmaswansneeded breather today oh godwent lunch collapsed like sack bones car',\n",
       " 'room full people lifting collide prayer excited friday night',\n",
       " 'maybe someday well find place dreams reality collide',\n",
       " 'im saying freak collide starts mins freak get',\n",
       " 'added video youtube playlist howie day collide',\n",
       " 'even stars moon collidenever wantback life mtvhottest justin bieber',\n",
       " 'collide gateway students make setting chairs tables fun thanwouldve thought collidenlc',\n",
       " 'perez hilton forever niley shipper lol',\n",
       " 'escape place hide time space collide',\n",
       " 'nigga first timeheard song justine sky collide',\n",
       " 'vamos newells',\n",
       " 'funfact facts stars collide',\n",
       " 'transgender sam takes runaway lizzie two different worlds collide explore unjust boundaries trustsamuelbooks',\n",
       " 'social media marketing ethics rules collide recorder',\n",
       " 'trump amp bill clinton collide best conspiracy story ever via motherjones trump democratic plant lmao lastword',\n",
       " 'pierce veil rubber bracelet wristband collide sky full read ebay',\n",
       " 'theres days collide get signed',\n",
       " 'farhankvirk ptisalarkhan instead promoting false news first get facts right road rage incident vehicles collided',\n",
       " 'want win arc collided emery lords release open intl thestartofemandyou giveaway',\n",
       " 'mollywoodagreedidnt know moved marketplacewoke report amp thought favorite worlds bolapm collided',\n",
       " 'currently reading see collided dresser waiting get read',\n",
       " 'edsheeran tf innit',\n",
       " 'running stairs bad idea full collided floor',\n",
       " 'maritime sand carrier shinto maru chemical tanker hoshin maru collided nautical miles honshu',\n",
       " 'oh god sending thoughts sastars head coach dan hughes left game stretcher danielle robinson collided',\n",
       " 'remember beau andcollided slip slide anddied',\n",
       " 'ok two favorite things collided one great ht literarycakes',\n",
       " 'call car vs fed ex truck head collision mile marker prayers families get scene',\n",
       " 'motorcyclist bicyclist injured denver collision broadway',\n",
       " 'motorcyclist bicyclist injured denver collision broadway least two people taken local',\n",
       " 'traffic collision ambulance enroute florin rd franklin blvd south sac',\n",
       " 'denver post motorcyclist bicyclist injured denver collision broadway',\n",
       " 'collision daaamn',\n",
       " 'marin trfc collisionunkn inj',\n",
       " 'riverside con con trfc collisionunkn inj',\n",
       " 'techerit anti collision rear gadget technology',\n",
       " 'anti collision rear innovation gadgets',\n",
       " 'mom fights recover hospital fiery car crash kids miss home',\n",
       " 'citation billings police officer caused broadwater crash',\n",
       " 'little angel banjo pilon dies skateboarding crash wamberal weekend old',\n",
       " 'lol guy whipping double yellow line mustang crash curb light pole sns',\n",
       " 'new post india train crash kills dozens',\n",
       " 'crash',\n",
       " 'crashmatrix themercedesxxx minivan',\n",
       " 'nonpower schools best shot crash college football playoff',\n",
       " 'hilarious look got hairity rainbow crash spitty pie spikebot',\n",
       " 'thieves crash dealership gate steal four vehicles killeen',\n",
       " 'time enjoy summer crash course two weekswill never take summer classes thats lie',\n",
       " 'portmoody clarke rd hill nb blocked crash past seaview traffic back kemsley take como lake gatensbury get around',\n",
       " 'natethewolf car vs motorcycle crash fairport nine',\n",
       " 'learn cpanel join cpanel crash course',\n",
       " 'sterlingknight joeylawrence abcfmelissajoey car crash pretty fun film melissaandjoeychat',\n",
       " 'news injured school bus crash bordentown details',\n",
       " 'crash new depression seeds next crisis already sown',\n",
       " 'husbands back edinburgh crashed one young adults got raging temperature others gone ive tea',\n",
       " 'rip whoevers face crashed front work wishcould helped',\n",
       " 'almost crashed listening walk moon butdont carelove',\n",
       " 'olhead crashed back another olhead car hopped like damnnnnfuckin rookie slow tf',\n",
       " 'thomicks isnt showing methink crashed',\n",
       " 'rosannebarr new documents suggest clintons email server may crashed fan mail qatar must crashed',\n",
       " 'bin laden family plane crashed avoiding microlight landing far runway three members',\n",
       " 'blizzardcs crashed right stream lol',\n",
       " 'pahandabear nethaera yup eu crashed p',\n",
       " 'news bin laden family plane crashed avoiding microlight landing far runway three member',\n",
       " 'aqgco pakistan says army helicopter crashed countrys restive northwest fox news',\n",
       " 'got ticket amp car got crashed man lifes great',\n",
       " 'pak army helicopter crashed mansehra pray departed souls',\n",
       " 'website malfunctioning php scripts working database crashed need unique script php pro rescue',\n",
       " 'army personnel including majors martyred helicopter army medical corps crashed near mansehra',\n",
       " 'last days proven popular bbc watched bake iplayer crashed due cricket commentary',\n",
       " 'pkadlik jojowizphilipp andhave crush rob thomas right viptickets imgunnashakeitallnight',\n",
       " 'brother said crush girl named danielle oh god',\n",
       " 'women crush sure',\n",
       " 'thinkmight crush lover boy',\n",
       " 'aramis crush pitch strop',\n",
       " 'secretly crush sing lmk lmfao',\n",
       " 'crush cars wrestling stars mmx racing featuring wwe appadvice designgeeks',\n",
       " 'freakingg playing candy crush',\n",
       " 'im relationship person thats whenhave crush someonealways think theyre going feel way',\n",
       " 'dudehave crush hockey players paul always give butterflieshes angel',\n",
       " 'biggest girl crush oomf',\n",
       " 'teambarrowman saying man crush',\n",
       " 'omghave huge crush looks necessarilydont think looks way',\n",
       " 'crushed mi run pace nike sportwatch gps nikeplus',\n",
       " 'neptunia girls nearly level ive crushed nonsrank colosseum fights major grinding happens soon',\n",
       " 'amyramponi theyre crushed doesnt set negative selffulfilling prophecy negative selftalk psychat',\n",
       " 'old man shuffled slowly ice cream parlor ordered banana split waitress asked crushed nuts said arthritis',\n",
       " 'dancingonhwy nothinglove seeing bears fans get hopes dreams crushed',\n",
       " 'one episode undo season kai joins ff ren beats aichi psy misaki gets crushed fuck',\n",
       " 'rt target best price avril crushed sheer curtainpanel curtains decor blinds shades',\n",
       " 'also mcutchen crushed',\n",
       " 'im getting hopes crushed',\n",
       " 'uribe crushed',\n",
       " 'doublejust crushed ball jays',\n",
       " 'got season funniest season ever hilarious gameofthrones crushed omg cantstoplaughing wow comedy',\n",
       " 'soul crushed damn cute',\n",
       " 'loraccee joenbc msnbc still gets crushed foxnews says little cable news viewers general dumb sells sucks',\n",
       " 'uribe crushed mets',\n",
       " 'brandonwarne mlb ball feet absolutely crushed',\n",
       " 'ball crushed',\n",
       " 'big papi great welcoming show crushed',\n",
       " 'spicenthingsup wtfcant love bitreally think thats feels massive crush got crushed',\n",
       " 'ghazalabaji exactly curfew like situation whensit dad',\n",
       " 'black african refugees israel attend kenye west concert due curfew black refugees',\n",
       " 'kids bedtime gotten later amp later nows good time point regular curfew would btsprep parenting',\n",
       " 'karliknox know crazy strict philip gave texting curfew bruh',\n",
       " 'curfewbeagle beaglefreedom pretty curfew',\n",
       " 'parents try put curfew youre almost',\n",
       " 'everybody like fuck curfew im sitting like ill bed',\n",
       " 'benznibeadel hehe likehahaha im kidding loveim gon na sleep na bc curfew',\n",
       " 'annaesterly dude mins past curfew give self break',\n",
       " 'big hanging curfew',\n",
       " 'dometufnellpark know curfew havoc fest sunday please',\n",
       " 'sexthemed ebooks given curfew germany',\n",
       " 'big hanging curfew',\n",
       " 'happy report quite tropics cyclone development expected time cbsmiami miamiherald',\n",
       " 'comes next episode scorpion cyclone scorpioncbs',\n",
       " 'revival cyclone football begins today september',\n",
       " 'appeal cardinal charles bo sdb cnua dear brothers sisters recent cyclone massive',\n",
       " 'cyclone hits skye',\n",
       " 'orochinagicom gigatech cyclone',\n",
       " 'severe weather bulletin typhoon hannaph soudelortropical cyclone warningissued pm',\n",
       " 'first time everything coney island cyclone',\n",
       " 'aliyeskii wquddin maungkyawnu drkhubybe nslwin ibrahimdubashi mdskar zarnikyaw cyclone',\n",
       " 'erpestaraint bitch girl popobawa revolves around cyclone',\n",
       " 'weather tropicalweather tropical cyclone tracking update aug tropicalupdate',\n",
       " 'hurricane typhoon matter one also cyclone',\n",
       " 'waige driving like badass scorpionpilot cyclone scorpioncbs',\n",
       " 'raleigh cyclone gear mountain bike sportinggoods',\n",
       " 'rt severe weather bulletin typhoon soudelortropical cyclone warningissued pm',\n",
       " 'red tropical cyclone alert china northern mariana islands aug utc',\n",
       " 'localeventcountdown curtain opens black cyclone shelby tickets amp info rstoday',\n",
       " 'may rodder super stock keeling mustang barracuda riviera cyclone',\n",
       " 'damage done recover negative social media update smexaminer',\n",
       " 'beforeitsnews called damage control dont fooled folks perps come plan b',\n",
       " 'lms frauncestavern named one historic properties granted nygovcuomo repair sandy damage',\n",
       " 'globepolitics extreme positions lack debate pandering hardright increase divides corrodes trust damage goodwill',\n",
       " 'limited damage put runs board nyy',\n",
       " 'never riding gaymaro would damage reputation',\n",
       " 'alpine city park vandalized damage',\n",
       " 'got first damage today fuckkkkkk',\n",
       " 'vzwsupport zero damage horrible product',\n",
       " 'gatorgmuzik fettilootch slanglucci oppressions greatest danger coming soon album',\n",
       " 'used really think danger gon na movie',\n",
       " 'dont think realistic scully finally home sfe wanted go danger hipsbeforehands',\n",
       " 'onceaho empire girl vote abella danger adult empire running first ever hunt emp alwaysaho',\n",
       " 'free book blow mind suspense danger anger love importantly',\n",
       " 'post todaytalk dangers interruptions annoy',\n",
       " 'theelmagoo gopteens foxnews pattonoswalt anyone actually plays drinking game serious danger alcohol poisoning',\n",
       " 'pukesmoothiedont understand would put us danger',\n",
       " 'mlpdelrim please tell might danger knew knew',\n",
       " 'devil wears prada still one favourite films',\n",
       " 'allen west obama clear present danger republic',\n",
       " 'fresenius saidput patients lives danger time whats happening like bigger violation ppe',\n",
       " 'jamesmaynot amusicvideoaday kingruckus theres lot businesses operate outskirts ampnever feel danger',\n",
       " 'unitedutilities happyreceived text parasite dangeri transplantee amp virus kill kidney',\n",
       " 'last retweethopedrop dead',\n",
       " 'thought dead google glass gets second coming though considered epic failure according makers recent patent',\n",
       " 'dead',\n",
       " 'possum problems dead ducks',\n",
       " 'hope ross amp val thats dead emmerdale summerfate',\n",
       " 'dead ass truth pay shit gone use',\n",
       " 'emmerdale summerfate intense cant believe vals dead',\n",
       " 'nah emmerdale mikeparractor ross aint dead know hes unconscious dramatic teases',\n",
       " 'ross better dead emmerdale',\n",
       " 'nooooo village hotty dead emmerdale',\n",
       " 'mikeparractor im believing ross isnt dead',\n",
       " 'mikeparractor ross deadshall never watch emmerdale ill heartbroken',\n",
       " 'dont reckon ross dead actually said pete killed brother ross survived much die',\n",
       " 'please tell ross isnt actually deadi hope theyre playing us next episode emmerdale',\n",
       " 'flood two people dead houses destroyed kaduna',\n",
       " 'youre boyfriends ex feelings dont talk dont follow dead dont exist niggah',\n",
       " 'everyone doesnt know chris prater struck train morning pronounced dead scene prayers family',\n",
       " 'tonedidit cant real nah mother dead wrong',\n",
       " 'way ross dead emmerdale',\n",
       " 'incredible character heart broken actually dead ripross miss',\n",
       " 'baeffee randompelar black death black death whoo',\n",
       " 'liked youtube video camp bunkd disney channel',\n",
       " 'obama dont worry good iranians kidding death america chants bad ones like republicans',\n",
       " 'microsoft xbox console rrod red ring death parts repair full read',\n",
       " 'headache death',\n",
       " 'itristontyler confession whenwas littlehad hamster room cold froze death oops',\n",
       " 'wall going death medue years water leaks lack maintenance',\n",
       " 'iran deal result terrorism death truth revolt',\n",
       " 'swearyg looks like death star',\n",
       " 'death certainty life',\n",
       " 'single death tradery many deaths statistics',\n",
       " 'spirituality born againspiritual awakening near death experiences charlesfrize frizemedia dynamicfrize',\n",
       " 'kaaypaat actually death takes way longer youd think',\n",
       " 'days whenlook like deathhave see mo city chick fil today',\n",
       " 'deaths',\n",
       " 'betrayedhunter reading paused came across couple deaths began read deeper',\n",
       " 'deaths',\n",
       " 'bigamist first wife charged deaths second pregnant wife child mother',\n",
       " 'health fact women account smoking deaths',\n",
       " 'series deaths seemed one right another book seems month pass death liz',\n",
       " 'bootlegalbano shaunkingknow profits deaths people claims fighting makes sick',\n",
       " 'mohammedzismail deaths regular gunsmissilesbombs warfare abombs used war',\n",
       " 'infections driving neonatal deaths health minister guyana',\n",
       " 'bigamist first wife charged deaths second pregnant wife child mother nephew u',\n",
       " 'msha stepping enforcement mining deaths via elkodaily',\n",
       " 'bigamist first wife charged deaths second pregnant wife child mothe',\n",
       " 'wing part enough solve mystery',\n",
       " 'debris found indian ocean could malaysia flight',\n",
       " 'bbc news reunion debris missing malaysia flight',\n",
       " 'news island wreckage missing malaysian jet officials debris found reunion island flight',\n",
       " 'hopefully bring closure families malaysian pm confirms debris missing flight',\n",
       " 'runion debris almost surely flight officials say',\n",
       " 'aus aircraft debris found island malaysia confirms abc',\n",
       " 'aircraft debris found la reunion missing malaysia airlines',\n",
       " 'authorities increasingly confident airplane debris recovered wednesday comes official analysis set begin',\n",
       " 'investigators plane debris conclusively gadgets tech',\n",
       " 'top story reunion debris missing plane bbc news see',\n",
       " 'per malaysias pm piece debris conclusively part wing right',\n",
       " 'interesting aircraft debris found la reunion missing malaysia airlines abc please rt',\n",
       " 'rmt playing jackson browne deluge listeners requests',\n",
       " 'liquidslap name ja rules best song lt guess song ive never heard nothinghave heard could possibly candidate',\n",
       " 'already happens abs rowers tweeting support nats last year deluge celeb endorsements obama',\n",
       " 'ciostrategyau applicable business warfare battles future decided data',\n",
       " 'euro ensembles much optimistic weekend even though operational counterpart deluge',\n",
       " 'im havin previous life flashbacks whenlived weimar berlin hustlin life unter der linden deluge',\n",
       " 'bigdata deluge',\n",
       " 'decadelong billiondollar deluge messages government canada harperslegacy',\n",
       " 'feel deluged unhappiness take quiz',\n",
       " 'deluged low selfimage take quiz',\n",
       " 'thousands displaced houses amp roads deluged floodwater week persistent rains floods climate cdnpoli',\n",
       " 'businesses deluged invoices make stand colour shape anq likely rise top pay pile',\n",
       " 'businesses deluged invoices make stand colour shape likkly rise top os pay pile',\n",
       " 'police still searching two people rain mud deluged western town tetovo deutsche sprache',\n",
       " 'businesses cre deluged invoices make stand colour shape likely rise top pay cile',\n",
       " 'watching deluged data doczone ipad cbc',\n",
       " 'businesses deluged invoices make stand colour shape likely rise top che pay pile',\n",
       " 'businesses deluged invoices make yoursstand colour shape likely rise top pay pipe',\n",
       " 'feel deluged unhappiness take quiz',\n",
       " 'businesses adeluged invoices make standwout colour shape likely rise top pay pile',\n",
       " 'businesses deluged invoicesx make stand ou colour shape likely rise top pay pile',\n",
       " 'brain deluged thoughts',\n",
       " 'businesses deluged invoices make stand olour share likely rise top pay pile',\n",
       " 'businesses deluged invoicew make stand colouj shape likely rise top pay pile',\n",
       " 'businesses deluged invoices make stand colour shape ikely rise pay pile',\n",
       " 'cant believe never occurred thatcould deluged kickstarter emails',\n",
       " 'businesses deluged ivoices make stand colour shape likely ris top pay pile',\n",
       " 'facebook deluged friending spam last days someone targeting reason',\n",
       " 'glimpses ltbgthyderabadltbgt deluged heavy rainfall hyderabad flood waters heading',\n",
       " 'deluged dismay sosoon surrendered summer droughtdescending disturbedsunrisehaiku poetry wimberley atxsmtx',\n",
       " 'horror community deluged cruddy ostensibly macabre photography dont contribute pandemic',\n",
       " 'bartonleclay house owner told demolish extension bbc news',\n",
       " 'rt abbswinston zionist terrorist demolish tire repair shop structure bethlehem',\n",
       " 'salvation army bid demolish cottages southend',\n",
       " 'enugu government demolish illegal structures international conference centre',\n",
       " 'contractor hired demolish goodrich plant miami files bankruptcy tulsa world manufacturing',\n",
       " 'enugu government demolish illegal structures international conference centre enugu state government app',\n",
       " 'demolish head corruptwill vandalise head aliyu mani secschoolinnigeria',\n",
       " 'listen anakasparian demolish case planned parenthoodwhiny voicejust really love aboooooortiooooonnnnns defundpp',\n",
       " 'hope someone buys former post office napa sale preservation',\n",
       " 'demolish team right eat pizza dont need fitness ashton irwin mtvhottest',\n",
       " 'trying get higher bathroom work pen go demolish food',\n",
       " 'year old ring fort demolished',\n",
       " 'enugu government demolish illegal structures international conference centre rt',\n",
       " 'economic wisdom great economists demolish establishment nonsense case logic reason',\n",
       " 'shane im sorry wrong propagating attack would demolish england reversal',\n",
       " 'promise winningwhit andcould assemble team demolish',\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pipeline = Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['text', 'target'],[ 'text'], False)),\n",
    "    ('preprocess', Preprocess(True))\n",
    "])\n",
    "\n",
    "pred_cleaned = pipeline.fit_transform(test)\n",
    "pred_text = list(pred_cleaned)\n",
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 3263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 3263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] happened terrible car crash [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] happened terrible car crash [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3047 6659 2482 5823 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3047 6659 2482 5823 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] heard earthquake different cities stay safe everyone [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] heard earthquake different cities stay safe everyone [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2657 8372 2367 3655 2994 3647 3071 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2657 8372 2367 3655 2994 3647 3071 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] forest fire spot pond geese fleeing across street ##can ##not save [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] forest fire spot pond geese fleeing across street ##can ##not save [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3224 2543 3962 8644 28519 14070 2408 2395 9336 17048 3828 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3224 2543 3962 8644 28519 14070 2408 2395 9336 17048 3828 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] apocalypse lighting spokane wild ##fires [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] apocalypse lighting spokane wild ##fires [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 16976 7497 21878 3748 26332 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 16976 7497 21878 3748 26332 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] typhoon so ##ude ##lor kills china taiwan [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] typhoon so ##ude ##lor kills china taiwan [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 15393 2061 12672 10626 8563 2859 6629 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 15393 2061 12672 10626 8563 2859 6629 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Exception ignored in: <generator object Estimator.predict at 0x7f9bb94b2fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 647, in predict\n",
      "    for key, value in six.iteritems(preds_evaluated)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5652, in get_controller\n",
      "    yield g\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5467, in get_controller\n",
      "    type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/model.ckpt-713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/model.ckpt-713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = getPrediction(pred_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happened terrible car crash',\n",
       " 'heard earthquake different cities stay safe everyone',\n",
       " 'forest fire spot pond geese fleeing across streetcannot save',\n",
       " 'apocalypse lighting spokane wildfires',\n",
       " 'typhoon soudelor kills china taiwan',\n",
       " 'shakingits earthquake',\n",
       " 'theyd probably still show life arsenal yesterday eh eh',\n",
       " 'hey',\n",
       " 'nice hat',\n",
       " 'fuck',\n",
       " 'nodont like cold',\n",
       " 'nooooooooo dont',\n",
       " 'dont tell',\n",
       " '',\n",
       " 'awesome',\n",
       " 'birmingham wholesale market ablaze bbc news fire breaks birminghams wholesale market',\n",
       " 'sunkxssedharry wear shorts race ablaze',\n",
       " 'previouslyondoyintv toke makinwas marriage crisis sets nigerian twitter ablaze',\n",
       " 'check',\n",
       " 'psa im splitting personalities techies follow ablazeco burners follow ablaze',\n",
       " 'beware world ablaze sierra leone amp guap',\n",
       " 'burning man ablaze turban diva via etsy',\n",
       " 'diss song people take thing run smh eye opener though set game ablaze cyhitheprynce',\n",
       " 'rape victim dies sets ablaze old girl died burn injuries set ablaze',\n",
       " 'setting ablaze',\n",
       " 'ctvtoronto bins front field house wer set ablaze day flames went rite hydro pole wonder',\n",
       " 'nowplaying alfons ablaze puls radio pulsradio',\n",
       " 'burning rahm lets hope city hall builds giant wooden mayoral effigy feet tall amp sets ablaze johnkass',\n",
       " 'philippaeilhart dhublath hurt eyes ablaze insulted anger',\n",
       " 'accident cleared paturnpike patp eb cranberry slow back traffic',\n",
       " 'got love burning self damn curling wandswear someone needs take away cuase im accident prone',\n",
       " 'hate badging shit accident',\n",
       " 'car recorder zeroedge duallens car camera vehicle trafficdriving historyaccident camcorder large',\n",
       " 'coincidence curse still unresolved secrets past accident',\n",
       " 'trafficsouthe roadpoleast accident near lewes kingston roundabout rather',\n",
       " 'sakumaen pretend feel certain way feeling become genuine accident hei darker black manga anime',\n",
       " 'legal medical referral service call us accident slipandfall dogbite',\n",
       " 'theres construction guy working disney store huge gauges ears bloody accident waiting happen',\n",
       " 'robynjilllian wlsdomteethsfeel like im going accident teesha gon na come',\n",
       " 'northbound junctions currently delays mins due accident',\n",
       " 'daveoshry soembie ifsay thatmet accident week would super jelly dave p',\n",
       " 'accident hit run cold block se vista ter gresham gresham police',\n",
       " 'happened accident butlike',\n",
       " 'please donate spread word training accident left polevaulter kira grnberg paraplegic',\n",
       " 'please like share new page indoor trampoline park aftershock opening fall',\n",
       " 'bxckylynch foi roh aftershock las vegas procura pirate bay que tem',\n",
       " 'schoolboy aftershock original mixexcision amp skism sexism far loud remixfirebeatz schella dear new',\n",
       " 'ir icemoon aftershock djicemoon dubstep trapmusic dnb edm dance ices',\n",
       " 'aftershock happened nepal last intl team still way responders chief collins lacofd',\n",
       " 'ir icemoon aftershock djicemoon dubstep trapmusic dnb edm dance ices',\n",
       " 'stop sayingwish start sayingwill unknown',\n",
       " 'want go aftershock october bandslisten nxt cant afford yet though gradschoolapps',\n",
       " 'still living aftershock hiroshima people still scars history edward bond',\n",
       " 'ir icemoon aftershock djicemoon dubstep trapmusic dnb edm dance ices',\n",
       " 'aftershock',\n",
       " 'victory bargain basement prices dwight david eisenhower',\n",
       " 'far best competitive maps imo hope invahnwetrust',\n",
       " 'brass copper cataclysm amp aftershock',\n",
       " 'jadeformkx happydont use aftershock variation counters play style hard',\n",
       " 'aftershock',\n",
       " 'carelessness leads aviation accident victim right seek compensation damages',\n",
       " 'rewindmusic found guys todayregarding accident airplane lol amp became fan sending love amp support cali',\n",
       " 'girl died airplane accident fifteen years ago',\n",
       " 'mintechan hihow keio line stationright btw know airplane accident near chofu airport week',\n",
       " 'mexican airplane accident ocampo coahuila mx killed coahuila government subsecretariat francisco garcia castells age',\n",
       " 'horrible accident man died wings airplane',\n",
       " 'god accident happen airplane idc rest luggage completely destroyed please save makeup',\n",
       " 'horrible accident man died wings airplane',\n",
       " 'update picture penn twp airplane accident',\n",
       " 'see judge ruled accident jfk airport involving korean air',\n",
       " 'thugiaurenhad airplane mode accident',\n",
       " 'years ago today baseball lost one greats airplane accident rip captain yankees yankees',\n",
       " 'experts france begin examining airplane debris found reunion island french air accident experts wedn',\n",
       " 'breakingnews experts france begin examining airplane debris found reunion island french air accident',\n",
       " 'experts france begin examining airplane debris found reunion island french air accident experts wedn',\n",
       " 'twelve feared killed pakistani air ambulance helicopter crash',\n",
       " 'im dying send ambulance mbelly brightasastar',\n",
       " 'ambulance sprinter automatic frontline vehicle choice lez compliant ebay',\n",
       " 'disneyxrowbrina ambulance isnt even parked well like nearly top someones car im laughing',\n",
       " 'shot times found dead cuffs involved car accident officers told ambulance treat',\n",
       " 'margaretcho call fag im going call ambulance rainbowpower',\n",
       " 'new nanotech device able target destroy blood clots science',\n",
       " 'want send medic supplies dads ambulance',\n",
       " 'held heroin ambulance',\n",
       " 'twelve feared killed pakistani air ambulance helicopter crash',\n",
       " 'helicopter ambulance ride transfer hospital miles away cost',\n",
       " 'privileged proud wear uniform nhs ambulance gayuk uniform proud privileged whatsyouremergency',\n",
       " 'bartender work described drunk man annihilated hsnowberger days',\n",
       " 'cop pulls drunk driver safety seconds car hit train via viralspell',\n",
       " 'cop pulls drunk driver safety seconds car hit train via viralspell',\n",
       " 'rosiegray sincerety think un would move israel fraction chance annihilated',\n",
       " 'johnboywest stage drama queen stand drama stage amp certainly never world stage would annihilated',\n",
       " 'drodrolagi handplacementgoals bro annihilated hashtag week',\n",
       " 'dostep outside seconds get annihilated mosquitoes',\n",
       " 'jacksfilms yiayplan well first strike dreamworks minions annihilated',\n",
       " 'nature appropriates love burn become annihilated beloved',\n",
       " 'ninahoag shred psych work friendship would annihilated',\n",
       " 'thehill example ofthe conservatives annihilated burtonwiimington prkng auth liberals stood amphave done nothing',\n",
       " 'aug kaiserjaegers wiped francis josephs crack regiment annihilated carso plateau',\n",
       " 'die everything annihilated',\n",
       " 'brooo got annihilated',\n",
       " 'albertbreer probably annihilated needed dd',\n",
       " 'gmcr longe rgreen mountain red mountainstock annihilated hours',\n",
       " 'please sign amp rt save saltriverwildhorses',\n",
       " 'allied plans annihilation german people louis nizer interesting',\n",
       " 'please share sign petition save wild horses arizona',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'new souls punished annihilation',\n",
       " 'latest updates salt river wild horse roundup via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'bosteenauthfest silent planet print annihilation audio',\n",
       " 'rt sign urgent stop annihilation salt river wild horses savewildhorses saltriverhorses',\n",
       " 'jackienatalydltdoonly get iced annihilation',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'incindia sonia gandhi like pakistan every defeat treated like victorytotal annihilation answer',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'stop annihilation salt river wild horses via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'us national park services tonto national forest stop annihilation salt river wild horse via change',\n",
       " 'spherulitic almost feels like si signs apocalypse lol',\n",
       " 'liked youtube video teamtwiistz futuristic house built survive apocalypse minecraft',\n",
       " 'liked youtube video minecraft night lucky block mod bob apocalypse wither amp mod showcase',\n",
       " 'one major reasons live big city rrs miss meals ever comes pass cue apocalypse script',\n",
       " 'askconnor theres zombie apocalypse item right weapon youre either screwed youre gon na live',\n",
       " 'apocalypse',\n",
       " 'itskingriffyall im excited apocalypse really dig xmen franchise goinglike event theme lot',\n",
       " 'tmfkco sounds like terrible time ill right',\n",
       " 'apocalypse artists go fukushima exclusion zone nuclear ura',\n",
       " 'southern prepper fictionsharecropping apocalypse apocalyptic doomsday prepper book reading',\n",
       " 'honestly could say apocalypse coming andwould exited hes enthusiastic everything',\n",
       " 'also nephew proof fat babies going save us apocalypse',\n",
       " 'liked youtube video zombie apocalypse rescue',\n",
       " 'added video youtube playlist futuristic house built survive apocalypse minecraft maps',\n",
       " 'live princes apocalypse dampd encounters meerkat',\n",
       " 'liked youtube video minecraft night lucky block mod bob apocalypse wither amp mod showcase',\n",
       " 'omg zombie apocalypse among students',\n",
       " 'liked youtube video minecraft night lucky block mod bob apocalypse wither amp mod showcase',\n",
       " 'universal order armageddon',\n",
       " 'patent pending stream armageddon ep',\n",
       " 'armageddon london',\n",
       " 'hitchbotheard might rise ashes armageddon go hitchbot like true superstar',\n",
       " 'turkey invades israel halfway armageddon',\n",
       " 'remember crowd around baggage carousel like armageddon bags last remaining food items earth animals',\n",
       " 'never compromise even face armageddonwatchmen',\n",
       " 'mustread vladimir putin issues major warning late escape armageddon pcraigroberts us rus',\n",
       " 'da one mtvsummerstar videoveranomtv mtvhottest britney spears lana del rey',\n",
       " 'build army lions leader dog fight lions die like dog',\n",
       " 'one direction pick fan army directioners',\n",
       " 'shark army black date stainless steel quartz men sport watch full read ebay',\n",
       " 'beyonce pick fan army beyhive',\n",
       " 'seconds summer pick fan army',\n",
       " 'beyonce pick fan army beyhive',\n",
       " 'victorinox swiss army date womens rubber mop watch',\n",
       " 'hope salvation army least gave tax receipt',\n",
       " 'one direction pick fan army directioners',\n",
       " 'one direction pick fan army directioners',\n",
       " 'one direction pick fan army directioners',\n",
       " 'check data upper wabash reservoirs louisvilleusace site',\n",
       " 'answertherefore came pass sixteenth yeardid go forth head army',\n",
       " 'one direction pick fan army directioners',\n",
       " 'seconds summer pick fan army',\n",
       " 'cheesehead report arson charges filed jackson county house fire',\n",
       " 'arson suspect linked fires caught northern california',\n",
       " 'israeli police unable solve case duma arson attack',\n",
       " 'shock report muslims setting wildfires across american westarson jihad',\n",
       " 'pnutsmama strong times like arson happens push thru past felony stage',\n",
       " 'breakingnews mourning notices stabbing arson victims stir politics grief israel posters rome',\n",
       " 'mourning notices stabbing arson victims stir politics grief israel posters shira banki',\n",
       " 'rt anopus elliott probation two years pay fines',\n",
       " 'rt saginaw police sketches persons interest saginaw arson',\n",
       " 'fire destroyed charlton winery ruled arson',\n",
       " 'wehtwtvwlocal trial date set man charged arson burglary',\n",
       " 'man charged connection shawnee arson burglary olathe kan man court wednesday accused',\n",
       " 'steele police arrest possible arson investigation',\n",
       " 'vixuhn rip arson',\n",
       " 'arson suspect linked fires caught northern california',\n",
       " 'rt theadvocatemag owner chicagoarea gay bar admits arson scheme lgbt',\n",
       " 'lgbtq news owner chicagoarea gay bar admits arson scheme frank elliott pleaded via theadvocatemag',\n",
       " 'mourning notices stabbing arson victims stir politics grief israel posters shira banki',\n",
       " 'contra costa co authorities arrest suspected serial arsonist authorities believe',\n",
       " 'owner chicagoarea gay bar admits arson scheme frank elliott pleaded guilty hiring arsonist',\n",
       " 'localarsonist cloudygoldrush man shit',\n",
       " 'mo way says carry',\n",
       " 'arsonist sets nyc vegetarian restaurant fire police newyork',\n",
       " 'arsonist torches house getaway vehicle police say surrey leader richardgearl',\n",
       " 'safyuanalready threw away letter tho',\n",
       " 'hotel arsonist caused damage crying help arsonist jailed court hear',\n",
       " 'suspected serial arsonist east bay arrest nightbeattv kbcwtv veronicadlcruz',\n",
       " 'arsonist mc christ walk toronto columbus gospel',\n",
       " 'bitch dont get slapped',\n",
       " 'trapvodka new fave vine',\n",
       " 'suspected serial arsonist arrested norcal',\n",
       " 'surveillance video captures man removing american flag long beach home ktla',\n",
       " 'serial arsonist gets bail jail release',\n",
       " 'arson suspect linked fires caught northern california',\n",
       " 'delhi government provide free treatment acid attack victims private hospitals aapatwork',\n",
       " 'im going panic attack',\n",
       " 'end innovation reduce attack surface industryinsights mcgsecure',\n",
       " 'realtwanbrown yesterdayhad heat attack whats funny relationship snapchat',\n",
       " 'texass gratuitously cruel attack dying gayman',\n",
       " 'cooper super pooper hero dog saved drowning detected heart attack beforeknew',\n",
       " 'double ebony attack blackhaired blowjob',\n",
       " 'suspect latest us theatre attack psychological issues',\n",
       " 'people tilly confused cat overcomes horrible attack win hearts',\n",
       " 'love love love massive attack angel hd va youtube',\n",
       " 'sunnyclaribel many dpjhodges articles attack labour defend conservatives see',\n",
       " 'rt rtcom tory attack freedom information assault govt accountability liberty',\n",
       " 'weirdhad near panic attack amp wanted write walked bathroom work nearly cried dreadfully miss',\n",
       " 'porn blacks attack white married slut',\n",
       " 'cop injured gunfight militants attack udhampur police post suspected militants attacked police post',\n",
       " 'newcastle schoolgirl attacked seaton delaval park pack animals led former friend newcastle',\n",
       " 'aus need career best along steve smith even try competing sura virus attacked team australia brutally',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'tennessee man killed police attacked family hatchet',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'attacked robotlvl ive earned total free satoshis robotcoingame bitcoin freebitcoin',\n",
       " 'attitude problem result constantly belittled bashed attacked amp demoralized fuck',\n",
       " 'mustve forgot memo getting attacked resident receiving concussion funny',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'logitechg fire nation attacked',\n",
       " 'christian attacked muslims temple mount waving israeli flag via pamela geller',\n",
       " 'metro nashville police simply best thank protecting us call via aol',\n",
       " 'peaceful happy twitter experience avalanche idiocy',\n",
       " 'car receiving tube installation avalanche sound output hhw',\n",
       " 'fucking truewill decapitated throw head avalanche',\n",
       " 'one glance avalanche drops one look heartbeat stops oh walk moon get mecouldnt said better',\n",
       " 'photo repost gregskazphotography repostapp rainierarms avalanche ambi charging handle',\n",
       " 'dad told would never let get avalanche',\n",
       " 'imaginehad plenty talk maths teacher liverpool similar age music',\n",
       " 'rare vintage collectable windriver avalanche edt spray mendealsuk',\n",
       " 'plus performance chip fuel saver chevy silveradoavalancheblazertahoe',\n",
       " 'dorion sens looking consistency hoffman coloradoavalanche avalanche',\n",
       " 'avalanche need anyone carry bag',\n",
       " 'musician kalle mattson recreates classic album covers clever music video avalanche',\n",
       " 'tix frozen fury xvii los angeles kingsavalanche rowl mgm grand',\n",
       " 'fc maid agency presents musician kalle mattson recreates classic album covers clever music video',\n",
       " 'avalanche rear axle assembly',\n",
       " 'senators another year determine hoffmanscoloradoavs nhlavalanche',\n",
       " 'manotwits kolaveriboy avast avalanche andals',\n",
       " 'colorado avalanche anaheim ducks tickets preseason',\n",
       " 'listen wtm avalanche take deep breath',\n",
       " 'theres avalanche men lunatics livingontheedge',\n",
       " 'environment lone pine trees growing across country remembrance battle gallipoli years ago',\n",
       " 'jackisheafferhave battle',\n",
       " 'exoticengram therasputin raspy cool already figured ttks battle shit',\n",
       " 'liked youtube video pokmon omega ruby alpha sapphire wifi battle vs',\n",
       " 'long battle spurs matt bonner took care brian scalabrine knockout vine nba',\n",
       " 'liked youtube video screwattack knuckles punches death battle',\n",
       " 'lonepine remembered around australia descendants grow via gallipoli',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps breakingnews',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps via usatoday',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps via usatoday',\n",
       " 'unsurprisingstill dismaying consequences went securing pathogen access',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'latest fallout biolab safety lapses fedex longer transport anthrax amp select agent research specimens',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'fedex stops shipping potential bioterror pathogens fedex corp nyse fdx longer deliver packages',\n",
       " 'citizens education project acted years watchdog dugway proving grounds',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'fedex longer transport bioterror germs via usatoday',\n",
       " 'offbeat news fedex stopped shipping bioterror pathogens',\n",
       " 'fedex longer transport bioterror germs wake anthrax lab mishaps newsdict news fedex',\n",
       " 'bioterrorism ebola rt stopislam tcot ccot makedclisten teaparty',\n",
       " 'largest bioterrorism attack us soil involved tacos atlasobscura',\n",
       " 'time hedge catastrophic risks climate change asteroid impacts bioterrorism',\n",
       " 'uscourt amp colluded take whtusagov auth hostageamp make look blk wbioterrorismampuse lglorg idis id still',\n",
       " 'bioterrorism guidelines medical public health management',\n",
       " 'bbcworld bioterrorism drones reach large populated erea',\n",
       " 'bishoyragheb fair bioterrorism way go mean bsc isnt irrelevant',\n",
       " 'debtassassin forget threat bioterrorism',\n",
       " 'hsts anthony kimery discusses bioterrorism syfy channels joe rogan questions everything pathogen virus',\n",
       " 'anthony kimery discusses bioterrorism syfy channels joe rogan questions everything',\n",
       " 'harvardu amp colluded take whtusagov auth hostageamp make look blk wbioterrorismampuse lglorg idis id still',\n",
       " 'uganda seen front line bioterrorism fight',\n",
       " 'ppl livinghiv charged aggravated assault bioterrorism thingslow risk transmission hivisnotacrime',\n",
       " 'studying anthrax bioterrorism carpediem',\n",
       " 'gacourts amp colluded take whtusagov auth hostageamp make look blk wbioterrorismampuse lglorg idis id still',\n",
       " 'bioterrorism guidelines medical public health management donald hen',\n",
       " 'fight bioterrorism sir',\n",
       " 'fight bioterrorism sir',\n",
       " 'prsnvns thats good hear mine could better butcan manage alright bioterrorism doesnt wait',\n",
       " 'mariashriver fultoninfocourt trip keep strangersblkampwhtfrom usg ssapress id forfeit due bioterrorism lookap',\n",
       " 'afrojazz ill add youdont even know talking',\n",
       " 'never got paid give fuckwe might well blaze another one',\n",
       " 'junesnowpaw yeah gim dat creamy white stuff',\n",
       " 'reaction determines',\n",
       " 'port coquitlam fire crews say electric towel warmer started four alarm blaze last week destroyed several businesses',\n",
       " 'liked youtube video ashens blaze atgames handheld mega drives ashens',\n",
       " 'craving slurpees',\n",
       " 'omgneed go like yesterday',\n",
       " 'ovoizic baddiemoneysign blaze',\n",
       " 'lithgow news homeless blaze near portland',\n",
       " 'blizzarddracosawlets eat blaze blazinsmasher',\n",
       " 'big buzzy john blaze jus kame home year bid lovethyfamily',\n",
       " 'cruisingblazing banging',\n",
       " 'bitches say im hotsay bitch im blazing',\n",
       " 'bryanvsbracey ustadium need blazing speedthink watt far impressive obj another quick little guy',\n",
       " 'checked blazing horse tattoo yelp',\n",
       " 'midnight songcry goes realityin realitywas lonelyeven erased blazing sun',\n",
       " 'le ever blazing',\n",
       " 'packed ready go meet kkarmstrongg portugal tomorrow bring blazing sunshine fun times bestie',\n",
       " 'old ashwo lady festac town delta bbm leaked pictures',\n",
       " 'enjoying shade tree sun blazing cool breeze west hollywood park',\n",
       " 'happening fdny rescue blazing thruand blasting air horn',\n",
       " 'night insomnia whole stop blazing shit real',\n",
       " 'satelite shot series wildfires northern california continue blazing least five fire compl',\n",
       " 'website hosting get blazing speeds professional management unlimited options nyndesigns',\n",
       " 'come join us tomorrowaugust transcendblazing trail diversified world marketing',\n",
       " 'old ashwo lady festac town delta bbm leaked pictures',\n",
       " 'blazingroselia california hot blazing even though wildfires going even big massive rocky fire',\n",
       " 'people understand cant bleeding tweet everyone timedoes head stop ignoring hardly ignoring',\n",
       " 'nepal house near tia bleeding intl airline companies dry',\n",
       " 'battlehorkwishcould watch gif lu account posted mundo bleeding jesus',\n",
       " 'leona lewis ever see video dinah singing bleeding love',\n",
       " 'tombrevoort bleeding cool read tom brevoort though may look pictures',\n",
       " 'cut open andkeep bleeding bleedinglove leonalewis acapella singer canadiansinger',\n",
       " 'ears bleeding bass',\n",
       " 'kriszellner triplemania promise villano bleeding del rio whooping trick myztiziziziziz',\n",
       " 'looks like reynolds montano coming need stop bleeding best performance far',\n",
       " 'imkanomate stabbing really affect get stab wounds bleeding skarlet pushed backwards',\n",
       " 'yeahhate catsjust tried pet cat hand bite marks scratch marks bleeding dogsarebetterthancats',\n",
       " 'aichmomanic wraps cloth wound still bleeding ill ok falls ground im going die',\n",
       " 'ears bleeding',\n",
       " 'dudeandpal bad eyes ears bleeding',\n",
       " 'wouldnt want quit even ifwas bleeding',\n",
       " 'idkimnotfunny quickly blew mom doesnt get popular',\n",
       " 'lmfao snap blew',\n",
       " 'blew socan get follow back',\n",
       " 'atlwtmgc damn blew',\n",
       " 'wow social media really blew shania twain concert',\n",
       " 'cieistea idk blew',\n",
       " 'im always one phone call away many people done blew chances',\n",
       " 'well tweet blew fast',\n",
       " 'well blew whilewas sleeping',\n",
       " 'funnychubbyguy poor white guy mom made pizza wanted maccheese blew school sad story',\n",
       " 'wow bro blew quick believe hes funny clever person times like',\n",
       " 'tokyodotcomdidnt either till blew mentions lol',\n",
       " 'fucking around google maps work andpulled boise blew minds yeah idaho isnt think',\n",
       " 'stadium blew',\n",
       " 'balloon thing rudewould popped balloon whoever blew ups face',\n",
       " 'thefierygrave yeah true wouldnt got exposed probably wouldnt went mlg blew',\n",
       " 'reason players ego cause cod champs thats esports blew players strictly look orgs',\n",
       " 'thda kicks antiblight loan effort memphis',\n",
       " 'westmdcountypa land bank targets first latrobe building property acquisition fight blight',\n",
       " 'lots talk susceptible shoot blight without trama',\n",
       " 'help fight northern corn leaf blight headline amp',\n",
       " 'colemcfadyean fuck cole',\n",
       " 'imagine old gods awakened start blight biomarkdarrah mikelaidlaw',\n",
       " 'stamford tightens blight law dilapitated barn sits halferect hickory road near int stamford topix',\n",
       " 'cleveland heights shaker heights fight blight via clevelanddotcom',\n",
       " 'keep great work councilman brian oneill',\n",
       " 'blight every day',\n",
       " 'michelgrabowy vegetablefarmer internet loserlike leave snide remarksme person blight upon face earth',\n",
       " 'ok important question inquisition fighting darkspawn one gets blight entire game',\n",
       " 'member post norwalk blight ordinance said success properties change hands',\n",
       " 'time ripe yet another banana blight bananas appeal slipping future bananas',\n",
       " 'red eye crossing oceanpeeling potatoes servile devotionblight',\n",
       " 'detroit made progress blight many burned shells houses remain',\n",
       " 'justinfenton per sun piece last yearthink opposite done urban blight monumentalized',\n",
       " 'im renewable energy windfarmsdo agree abbott horrible blight landscape',\n",
       " 'buying mop',\n",
       " 'lonewolffur slaps',\n",
       " 'literally lunchhad cheese curds dinnerhad blizzard pls pray health',\n",
       " 'ohsee tied sweet blizzard',\n",
       " 'check new ebook sale timzak lizard stuck blizzard rhyming kidsbook amazon',\n",
       " 'deck building year old daughter',\n",
       " 'eddietrunk blizzard ozz',\n",
       " 'gunnar optiks heroes storm siege gaming glasses onyxfire wow blizzard full',\n",
       " 'game drove rolo mini blizzard',\n",
       " 'horrible moment whenopen dryer looks like snowy blizzard cuzleft piece paper jeans pocket',\n",
       " 'blizzard another month sub thing gave next wow expo free would probaly knock sub count pretty fucking high',\n",
       " 'internet sending subliminal messages ice cream thereforeneed blizzard rn',\n",
       " 'whoa new legend fun',\n",
       " 'louistomlinson ifrelate billie jean ur gon na also enjoy blood dance floor buybloodonthedanceflooronitunes',\n",
       " 'xoxoxxxooo rutinaofficial nbchannibal hannibal asks seen blood',\n",
       " 'bring horizon blood stained giant art print panel poster',\n",
       " 'aint crip bloodim doin thing',\n",
       " 'bitch im monster good blood sucker',\n",
       " 'criess blood golden wook life',\n",
       " 'blood makes related loyalty makes family sirbeezalot',\n",
       " 'youngthug remind blood sosa bellydo homework',\n",
       " 'yankees reason im blood pressure medicine',\n",
       " 'cant gay men donate blood',\n",
       " 'new mashup bad bloodwhere arenow comes tomorrow make surevine shaylencarroll',\n",
       " 'bloods penny post philly octagonal cds black grid cancel',\n",
       " 'seeing blood surgery regular gp behind desk say dentists arent real doctors ok',\n",
       " 'sometimes blood aint thicker water sometimes family bring quicker strangers',\n",
       " 'name chizugender maleage hair redeyes pinkdere type pasokonblood type type',\n",
       " 'whole new zealand shouting bloody marvellous john campbell join radio nz radionz',\n",
       " 'whats bloody use bus app everything prediction one driving bus today',\n",
       " 'continues best guest iloveggletters weeks episode bloody outrageous',\n",
       " 'bloody hot room',\n",
       " 'im bloody excited see maisy martha',\n",
       " 'jeremyclarkson buy new one bloody ell',\n",
       " 'youve twitter since bloody hell veteran',\n",
       " 'awful refereeing ruined balance game guy bloody mess imfc',\n",
       " 'precious lunch bloody hell brain issit',\n",
       " 'swear eyes bloody red bitchfeel amazing',\n",
       " 'shtballplayrsdo anyway running mouth bloody game pree sure youre bigger baseball calm hell',\n",
       " 'bloody nurses waking wheneventually fall asleep lol hospital',\n",
       " 'halfpeachh caitlincavanahcant deal right omg remembers bloody mary',\n",
       " 'nightmare elm street getting remade via bdisgusting',\n",
       " 'bloody hell teen wolfhayden liam cute andthink stiless jeep dead huh pleeeaseloved mtvteenwolf',\n",
       " 'luckily im cause mexicans came cut grass early andwas sleepwoulda full blown attitude',\n",
       " 'bored shit dont nobody fuck wit busy yo shit get blown smh',\n",
       " 'yes know youre adultressand yes killary emailslike smoke blown ur azznsa',\n",
       " 'sassynreal agreed whole thing blown faces right wrong indifferent public relentless court public opinion',\n",
       " 'dont understand white iverson post malone hasnt blown fuck already',\n",
       " 'jaykpurdy shush im getting blown poster size',\n",
       " 'license picture blown absolutely terrifying',\n",
       " 'deadly suicide bombing hits mosque saudi arabia suicide bomber blown mosque',\n",
       " 'phone blown rn',\n",
       " 'dodgersnation due get blown least still winnable game',\n",
       " 'lock last remaining shred manhood accept youre full blown sissy fe',\n",
       " 'get blown delicious flavor blowvape strawberry cherry pop try today',\n",
       " 'run singles rain blown',\n",
       " 'damn lastwitchhunter blown twitter feed looks amazing',\n",
       " 'malikchaimaahope zayn gets blown drone attack whilst visiting family pakistan',\n",
       " 'coldwood blown away support even hung art',\n",
       " 'bmrow first thought blownup dvd im pretty sure post mortem years back',\n",
       " 'new ladies shoulder tote handbag faux leather hobo purse cross body bag womens full read',\n",
       " 'cute soft washed fabric camouflage outdoor shoulder crossbody bag pack purple camouflage',\n",
       " 'cross body bag purse zippers shoulder bag camouflage camo phone case camera case wallet wallet wallets',\n",
       " 'louis vuitton monogram sophie limited edition clutch cross body bag full read ebay',\n",
       " 'nikon mp digital slr camera body batteries carry bag charger',\n",
       " 'auth louis vuitton brown saumur cross body shoulder bag monogram full read',\n",
       " 'dracula vampire cross body small shoulder bag',\n",
       " 'new diesel willyt printed front cross body shoulder bag one size ksl',\n",
       " 'new ladies shoulder tote handbag women cross body bag faux leather fashion purse full',\n",
       " 'nikon mp digital slr camera body batteries carry bag charger',\n",
       " 'new ladies shoulder tote handbag faux leather hobo purse cross body bag womens',\n",
       " 'would love win suzes amazing filled beauty bag giveaway contents loccitane body shop amp lavera',\n",
       " 'pl parenthood employee laughs aborted baby body parts mixed together bag',\n",
       " 'new ladies shoulder tote handbag faux leather hobo purse cross body bag womens',\n",
       " 'photo bonesymcbones beartooth body bag',\n",
       " 'twotone crossbody smartphone bag shipped via dians daily deals twotone',\n",
       " 'genuine leather man bag messenger fit ipad mini tablet case cross body air jp full rea',\n",
       " 'meekmillthink time consulted kendricklamar cause drake done body bagging',\n",
       " 'people bagging rouseys body shit id love body like ridiculing probably dudes skinny jeans',\n",
       " 'bagging last spot body attack getin traintuesday lesmills',\n",
       " 'dear lord forgive body bagging meekmill twice amen drake',\n",
       " 'otooledalton first cant hit stick shit second im body bagging sight',\n",
       " 'drake really body bagging meek atm',\n",
       " 'remember halo interview abc wasnt time pamanian devil body bagging fools',\n",
       " 'jerseys artist wolfgangjoc pays homage late great biggie smalls body bagging hypnotize beat',\n",
       " 'drake kill meek mill startedbagging dead body',\n",
       " 'man start body bagging local jokal photographers',\n",
       " 'body bagging',\n",
       " 'drake body bagging meek',\n",
       " 'body bagging thatthink time bring bags',\n",
       " 'need stop paying attention drizzy body bagging meekmill worry bout happen sandrabland',\n",
       " 'hear north niggas body bagging never know got spitters south',\n",
       " 'eish even drake killing niggas eish game really watered singing nigga body bagging rappers smh',\n",
       " 'esmcampy used werewolf also idiotawas tea bagging body like minutes fighting someone else',\n",
       " 'body bags movie review via youtube',\n",
       " 'lenngonewin nah man need defenseless thugs cops get get put cool new body bags',\n",
       " 'complexmag asking body bags pushat',\n",
       " 'rt saudi coming home yemen body bags saudi kingdom ego youre loo',\n",
       " 'hot ladies handbag leather shoulder tote satchel messenger cross body bags green handbags',\n",
       " 'fairfax co investigating firefighter khalil abdulrasheed facebook post saying police shld put body bags',\n",
       " 'mistakes come back body bags leading vuca world armed forces raghu',\n",
       " 'womens stylish metal chain pearls solid hanbags shoulder cross body bags golden',\n",
       " 'womens flower printed shoulder handbags cross body metal chain satchel bags pink',\n",
       " 'one top rted tweets japan screenshot young person claiming know anniversary hiroshima bomb',\n",
       " 'hiroshima marks years since bomb japanese city hiroshima marks anniversary dropping',\n",
       " 'joannabomb eventchase mtv mtvnews last year atw theyre multi platinum artists',\n",
       " 'peytonlaynec ohhh coming agehad bomb teacher never anything final went salt grass bought us food',\n",
       " 'bestfriends bomb',\n",
       " 'rip rt ajenews aug tokyo time worlds atom bomb hiroshima',\n",
       " 'joshrogin jimsciutto natseccnn suspected kerry barry got taken ride iran making bomb talks went',\n",
       " 'jonathanshaininthink bomb raises sorts great questions isnt one',\n",
       " 'american censors supressed accounts suffering nagasaki',\n",
       " 'hometown hit hiroshima atomic bomb safemode',\n",
       " 'johncusack text dropped abomb',\n",
       " 'bruh sex beachis bomb fuck yo',\n",
       " 'hansel gretel witch hunters bomb ass movie yehkno im late',\n",
       " 'ltkyodonewsgt hiroshima marks abomb anniv amid fears eroded pacifism followme japan',\n",
       " 'mattielbreauxthe bomb',\n",
       " 'aunt gave drink game bomb ill never tell though shhhh',\n",
       " 'tell shinichi kudo im giving minutes enjoy moriya teiji famous architect time bombed skyscraper',\n",
       " 'photo bombed princess',\n",
       " 'get call back interview thought bombed gtgtgt',\n",
       " 'hour yrs ago one greatest acts mass murder world history occurred hiroshima bombed',\n",
       " 'years ago nowish america bombed hiroshima justified allcan say nomorehiroshimas fepow',\n",
       " 'hiroshima look like bombed city looks monster steamroller passed squashed existence',\n",
       " 'july hiroshima bombed nuclear weapon bomb named little boyampcarried plane called enola gay',\n",
       " 'allenpeacock deal cleariran either agrees deal abandons program gets bombedits deal right wing love',\n",
       " 'picture cute sleeping puppy photo bombed demon back',\n",
       " 'johnejefferson remind somaliaeritrea amp nigeria bombed uk asylum applicants uk fromamp e',\n",
       " 'sitctips vlog get videobombed',\n",
       " 'bombed fake bolt chat everyone thought bolt ended lmfao',\n",
       " 'bombing hiroshima',\n",
       " 'dont really talk murder bombing campaigns wwii murder',\n",
       " 'hiroshima marks anniversary atomic bombing via nhk world news',\n",
       " 'japan mark anniversary hiroshima atomic bombing',\n",
       " 'happened asia japan marks anniversary hiroshima atomic bombing',\n",
       " 'like survive atomic bombing hiroshima',\n",
       " 'anniversary hiroshima bombing',\n",
       " 'japan marks anniversary atomic bombing hiroshima',\n",
       " 'japan marks anniversary hiroshima atomic bombing associated press via nyt',\n",
       " 'sanitised narrative hiroshimas atomic bombing japan',\n",
       " 'japan marks anniversary hiroshima atomic bombing bells tolled hiroshima thursday japan marke',\n",
       " 'japan marks anniversary hiroshima atomic bombing',\n",
       " 'yrs since atomic bombing hiroshima terrible mass murder',\n",
       " 'breaking news australia collapse hapless trent bridge',\n",
       " 'project syndicate marshall plan united states',\n",
       " 'new two giant cranes holding bridge collapse nearby homes',\n",
       " 'akinwunmiambode urgentthere currently storey building church bstop oworoshoki third mainland bridge likely collapse',\n",
       " 'throwback thursday thoughts citizen media minneapolis bridge collapse disaster tbt',\n",
       " 'ashes australias collapse trent bridge among worst history england bundled australia',\n",
       " 'two giant cranes holding bridge collapse nearby homes',\n",
       " 'throwback thursday minneapolis bridge collapse amp citizen journalism tbt',\n",
       " 'berggrueninst berggruen institute member dambisamoyo problems us infrastructure prosyn',\n",
       " 'australia collapse trent bridge twitter first morning fourth ashes test',\n",
       " 'shane warne ricky ponting shocked australia batting collapse trent bridge',\n",
       " 'injured missing bridge collapse central mexico',\n",
       " 'australias ashes disaster collapse unfolded trent bridge telegraph',\n",
       " 'sport shane warne ricky ponting shocked australia batting collapse trent bridge australian grea',\n",
       " 'injured bridge collapse swiss kosher hotel accident occurred tuesday metropol hotel',\n",
       " 'scary thought happened today would teenagers taking selfies front burning buildings',\n",
       " 'sounds like song would hear movie walking away burning buildings cars shit',\n",
       " 'ctvkathyle news dont run burning buildings',\n",
       " 'forestservice rt dhsscitech firefighters run burning buildingswe work tech',\n",
       " 'like folks said burning buildings doesnt workonlyit cities trying get information asap prevent',\n",
       " 'like ppl burning buildings knowing originally created',\n",
       " 'kou like cash register buildings burning',\n",
       " 'thehill hate crimes looney libs always burning buildings vandalzing private property',\n",
       " 'rules long life dont go burning buildings dont jump perfectly good airplanes dont awaken sleeping giants',\n",
       " 'port coquitlam fire burning several businesses',\n",
       " 'speechgirlam par caring sick helping people move talking movies rescuing people burning buildings',\n",
       " 'travdave kornbreadicu especially happened really nothing tear burning buildings saves city money real',\n",
       " 'fire brigade play guys music burning buildings put flames',\n",
       " 'two buildings burn antioch fire two buildings burning fire delta pines',\n",
       " 'denisleary sure folks rush burning buildings im grateful trueheroes',\n",
       " 'jane kelsey fire economy aug old govt buildings wgtonthe context amp driver tpp tradeinservicesagreement',\n",
       " 'failure doesnt exist change direction alejandro jodorowsky via buildingsonfire',\n",
       " 'im security want help case emergency like buildings fire shooters building im leaving tho',\n",
       " 'blainescronuts thats whatwould buildings fire',\n",
       " 'fucking apartments across street fire right fuckin firetrucks wtf second time buildings caught fire',\n",
       " 'testimony human spirit us dont set fire office buildings sunday night',\n",
       " 'multiple buildings fire downtown hinton okthe fire threatening level entire block',\n",
       " 'dnrres buildings shelled amp fire gorlovka tonightcivilian casualties janjuly killed incl children wounded',\n",
       " 'gilsimmons lightening struck amp units buildings development fire groton ct check fb wall recent post',\n",
       " 'taizhouthi saleh indiscriminate shelling castle sets tahreer st buildings fireyemen',\n",
       " 'buildings fire behind tisas niceville tristapnwfdn',\n",
       " 'two buildings involved fire street manchester wmur',\n",
       " 'agricultural buildings fire aldwark nyorks fire amp rescue service still grim news fewer piglets caught',\n",
       " 'multiple firefighters hospitalized fighting fire manchester nh',\n",
       " 'multiple buildings fire downtown hinton okthe fire threatening level entire block',\n",
       " 'happy pensioner storiessafe buildings watching report unsafe cladding catching fire melbourne',\n",
       " 'multiple housesbuildings fire kalispell',\n",
       " 'family heartbrokenmobile home nellis amp cheyenne burned fire kids ready go back school',\n",
       " 'burned fingers',\n",
       " 'burned stake',\n",
       " 'let tell brandace brought today ntc heat allwe survived burned',\n",
       " 'irsolichin potus last cities everyone burned death terrorism grandest scales',\n",
       " 'kyraelizabethh back break light burned sogotta show next days',\n",
       " 'realdonaldtrump burned republicans many times least talking things important us good luck',\n",
       " 'burned calories minutes total body cardio myfitnesspal',\n",
       " 'corey white talking dat shit got burned curlin irons black women necks',\n",
       " 'blackhatwriter yeah im going home computer next two weekends getting really burned really quick',\n",
       " 'pdleague im sorry crit sure matter prob cuzwas burned',\n",
       " 'got rhymes bibles got psalmsand like prodigal son ive returnedanyone stepping youll get burned',\n",
       " 'widouglass kurrrlyw adamrubinespn nationals welp dunce got burned',\n",
       " 'omg last weekaccidentally burned cigarette lol onlywould',\n",
       " 'adamrubinespnknow burned week ago similar spot getting outjust pitches save inn',\n",
       " 'keep calling crazycausenever learnedyou stop loving firebecause got burned',\n",
       " 'im proud heart played stabbed cheated burned broken somehow still works',\n",
       " 'hermancranston wired fires burning firefighters budgets',\n",
       " 'psachopped jalapenos amp fingers burninglavaonfire painful hrs first wear gloves peppers hell',\n",
       " 'geoengineering burning fossil fuels making global weather unstable temps biggest flood past years',\n",
       " 'sargiuliani im currently burning skin',\n",
       " 'yes comrades sweatseeking calorieburning cardiotorture need support get smile nod wave',\n",
       " 'wow legion slouching towards broken isles warlords draenor wasnt close enough burning crusad',\n",
       " 'thevenskus led killer workout today thesweatshoppe core burning gymtime worktime auditiontime roccotime nobs',\n",
       " 'peddle floorboard end four door burning backroad song park pile baby watch step',\n",
       " 'burning bridges forte',\n",
       " 'rt real burn book entry ca fuck place always burning',\n",
       " 'fear fever feel nowfeel fire burning beloooow gon na trick ya swallow whooooole mtvhottest justin bieber',\n",
       " 'burning legion returned',\n",
       " 'arab autocracies burning house via theeconomist',\n",
       " 'tarekfatah burning enemity pakistanm sure burn',\n",
       " 'put ton sunscreen im still burning',\n",
       " 'blizzard details new dungeons raids world warcraft legion burning legion',\n",
       " 'accomplishing burning killing buildings',\n",
       " 'amp common breaker trip threshold protect awg wire commonly used buildings burning power outlets results',\n",
       " 'burning buildingskeep flames lit',\n",
       " 'threealarm fire burning antioch apartment complex',\n",
       " 'rockbottomradfm one challenges tough enough rescuing people burning buildings',\n",
       " 'mrjamesob ure getting rid every last drop life gonecrawl burning buildings life matters jakey',\n",
       " 'firefighters brave people opinion bcclaredont knowcould save ppl burning buildings housessalute',\n",
       " 'rocky fire burning ca still threatening buildings largest state amp one nearly dozen burning',\n",
       " 'liked youtube video mega man zx ost ogre claw area burning buildings',\n",
       " 'denisleary sure folks rush burning buildings im grateful trueheroes',\n",
       " 'trubeque destruction magics fine dont go burning buildings',\n",
       " 'high skies burning buildings nowplaying',\n",
       " 'used drones help firefighters lead people burning buildings help put fire',\n",
       " 'attack pp attack womens health amp attack womens health attack americas health',\n",
       " 'soaring temperatures southern us states causing bush fires pity scotgov allow rain scotland tweetlikekez',\n",
       " 'know hot bush fires right near villa',\n",
       " 'bushfire causes first victim albania multiple fires albania caused first vict bush',\n",
       " 'drove past field fire yesterday reminded bush fires used get south africa',\n",
       " 'bush food fires',\n",
       " 'watch mother nature halts australia blue mountains bushfire',\n",
       " 'jiminswangwas say bush fires aus reason hot',\n",
       " 'bushfire causes first victim albania multiple fires albania caused first vict bush',\n",
       " 'like goats dislike bush fires probably video',\n",
       " 'pamelamueller time honour privlege prayhave witnessed bush fires living australia',\n",
       " 'therealpbarry much erupting volcano bush fires raging put atmosphere none believers mention',\n",
       " 'eugene fires got ta make think hashtag conspiracies hashtag population control hashtag bush',\n",
       " 'warfighting robots could reduce civilian casualties calling ban premature',\n",
       " 'lets hope concert ends zero casualties amen',\n",
       " 'american weapons support fueling bloody air war yemen',\n",
       " 'women children casualties afghan violence',\n",
       " 'gtiso fair enough change always unfortunate casualtieslike nzmorningreport utilises welly amp aucklnd well see',\n",
       " 'memorial day years hiroshima nagasaki august wth total casualties god forgive',\n",
       " 'warfighting robots could reduce civilian casualties calling ban premature',\n",
       " 'esksf always casualties right thing especially going cost boss money',\n",
       " 'joshdaniel mikegiarrusso foxnews think bunch untrained people firing back isnt going cause casualties',\n",
       " 'obama lambasted respected amp jokewould trump prefer civilian casualtiesgopmaddow',\n",
       " 'jimwinnm kurtschlichter heres thing predicting million american casualties forget goddamn japanese',\n",
       " 'american weapons support fueling bloody air war yemen',\n",
       " 'lets appreciate group people everyone hates much police prevented mass casualties today nashville',\n",
       " 'dontestallworth stevesgoddard look casualties iwo jima okinawa numbers dead sides youll see',\n",
       " 'cameronsurname allies forecast invasion japan minimum million casualties minimum',\n",
       " 'rxcasualty vpzedd dainttm call',\n",
       " 'legaljobs litigation attorney scottsdale az claims specialist ii casualty bodily injury jobs',\n",
       " 'social casualty',\n",
       " 'maccocktail first casualty war truth hiram johnson died day tony blairs case died war',\n",
       " 'georgian wines become casualty russias political campaign ukraine',\n",
       " 'bruminthecity oh coffee espresso poss casualty todays busiest thurs ever come perfect coffee house',\n",
       " 'associated agencies inc hiring property amp casualty account manager jobs rollingmeadows insurance',\n",
       " 'documenting climate changes first major casualty via greenharvard',\n",
       " 'canceling deal warships france agrees repay russia via nytimes',\n",
       " 'charlie casualty ashes',\n",
       " 'keep alive sound intensity level catering keeping auditory effect casualty insurance hcxvneoav',\n",
       " 'give money hear social casualty live mtvhottest buck id give',\n",
       " 'vpzedd reach skype',\n",
       " 'please read casualty fanslet us know amp help spread word retweeting dm scriptettesar info',\n",
       " 'business casualty',\n",
       " 'datshemmings moi cest plutot disconnected reason close strangers heartbreak girl rejects social casualty ufhguhfidt',\n",
       " 'fox news biggest media catastrophe american history people love living lie',\n",
       " 'twist sony ultimate preparedness library prepare catastrophe',\n",
       " 'buzzfeed stannis evil books got catastrophe saying',\n",
       " 'shaneryanhere catastrophe amazon prime',\n",
       " 'youa wonderful person thomasistrash thomasistrash',\n",
       " 'nowam quietly waiting catastrophe personality seem beautiful interesting modern',\n",
       " 'gaza summer catastrophe gaza palestine israel bds',\n",
       " 'caradelevingne mother chucker catastrophe much hotness one room cant cope swifties',\n",
       " 'twlldun peterjukes good piece andagree failure postwar planning catastrophe',\n",
       " 'sorry kylie pretty sure james ate bruce morning catastrophe',\n",
       " 'robdelaney desperate watch last episode catastrophe canget uk',\n",
       " 'barackobama hello mr president really big problem puerto rico regarding water situation like catastrophe',\n",
       " 'alaskas wolves face catastrophe alexander archipelagowolves population plummeted yr voice',\n",
       " 'two shows ive loved summer catastrophe unreal',\n",
       " 'vaunwilmott paystara agrees catastrophic loss love barn scene short shirtless michael dominion',\n",
       " 'something catastrophic coming tune',\n",
       " 'wp following catastrophic event united states decimated president prepares first post apocaly',\n",
       " 'johngreen catastrophic history',\n",
       " 'rise wages going catastrophic impact wage structure fuck automation going',\n",
       " 'unitednyblogs insidecityhall nyclass ouch needs explain nyclass hate mailer catastrophic fail',\n",
       " 'catastrophic effects hiroshima nagasaki atomic bombings still felt today',\n",
       " 'society respond looming crisis uncertain timing catastrophic proportions',\n",
       " 'dear carlyfiorina anyone family alive climatechange gopdebate feelthebern',\n",
       " 'cancer everything seems catastrophic feel worse hit rock bottom recover start got',\n",
       " 'jim rickards blog catastrophic outcomes may come faster expecte',\n",
       " 'stop extreme regulations hurt middle class',\n",
       " 'laladeviluke consequences could catastrophicwas also informed security ai mute primarily',\n",
       " 'gaming learning legacy catastrophic eruption fifteenmilehigh plume ash eruption wh',\n",
       " 'want sell boat car truck',\n",
       " 'long baby blissfully unaware catastrophic day happy fed bathed cared',\n",
       " 'suffered catastrophic pants failure work todayresorted tying jacket around waist tilgot thrift store trustory',\n",
       " 'society collapse due catastrophic food shortages says study',\n",
       " 'dear sentedcruz anyone family alive climatechange gopdebate',\n",
       " 'something catastrophic coming tune via post',\n",
       " 'chemical brothers play armory sf tomorrow night emergency bay area edm announcement chem',\n",
       " 'fake explosion toxic chemical drills emergency responders pope francis million visit sept',\n",
       " 'emergency crews scene chemical spill spohn shoreline details come us',\n",
       " 'standard behavior chemical mixers emergency purposesjyb',\n",
       " 'emergency services called bacup strong chemical smells',\n",
       " 'rossendale emergency services called bacup strong chemical smells rossendale',\n",
       " 'emergency services hammondville near jewell close story liverpool',\n",
       " 'chemical spill house hammondville emergency services attending breakingnews',\n",
       " 'bendwavy emergency chemical rinse',\n",
       " 'emergency services called bacup strong chemical smells',\n",
       " 'pmbreedlove russian nuclearbiologicalchemical nbc brigade emergency response exercise southern md',\n",
       " 'emergency prompted due chemical spill factory near redruthuk',\n",
       " 'god pushes edge cliff two things may happen catch fall teach fly',\n",
       " 'photographer brian ruebs endures climb capture bride groom rarasathie',\n",
       " 'might go losing itdrive cliff fall void',\n",
       " 'hope fall cliff',\n",
       " 'spencervhhope fall cliff',\n",
       " 'usetheiight youre ugly piece trashhopefall cliff',\n",
       " 'huge cliff landslide road china watch moment cliff collapses huge chunks rock fall onto',\n",
       " 'boy falloffcliff still got man rumor going back de',\n",
       " 'video man rescued cliff fall shaldonread',\n",
       " 'mattbuek pitchers gracefully lose stuff fall cliff land throwing arm hard tell gets',\n",
       " 'johnnieguilbert jump cliff ill cheering back flip fucks sakehope fall break neck',\n",
       " 'people im camping withwould happily see fall cliff',\n",
       " 'mega keen hiking tomorrow hopedont fall cliff haha keen fitness yay excitedmuch',\n",
       " 'ntsouthwest looks tasty problem cliff might fall sea due coastal erosion would crumbly',\n",
       " 'interview collapse materialism best talkradio listen live',\n",
       " 'greeces tax revenues collapse debt crisis continues',\n",
       " 'correction tent collapse story hnlnow',\n",
       " 'correction tent collapse story',\n",
       " 'knew cain wasnt going die either val someone gon na collapse later emmerdale summerfate',\n",
       " 'interview collapse materialism best talkradio listen live',\n",
       " 'west side water main break causes street collapse video el paso times',\n",
       " 'fouseytube zaynmalikwould collapse',\n",
       " 'guardian greeces tax revenues collapse debt crisis continues guardian fresh',\n",
       " 'oikawa karasunoguess id collapse overwhelming happiness',\n",
       " 'residents set sign tribute families lost lives thane building collapse unfortun',\n",
       " 'technical collapse gt wlb new low',\n",
       " 'arnejungjohann energiewende directly responsible collapse energy value',\n",
       " 'jfb tfb correction tent collapse story smantibatam',\n",
       " 'ashes rampant england humiliate rival australia',\n",
       " 'greeces tax revenues collapse debt crisis continues talks continue proposed third bailout',\n",
       " 'ppact hasnt already collapsed testament thedemocrats lockstep devotion politics reality',\n",
       " 'sheriffclarke carpenter built house collapsed would ask carpenter rebuild',\n",
       " 'memorial honoring died bridge collapsed',\n",
       " 'used good time june july summer collapsed smh',\n",
       " 'remember whenwas little anddidnt want say pledge school collapsed pit hell opened',\n",
       " 'ceiling walls collapsed upon darknesswas trapped',\n",
       " 'collapsed bathroom bcuz michael',\n",
       " 'video link billowing fire southtowne lanes roof collapsed',\n",
       " 'lashak quinlo lsedig yemmely inmianajonessaw crazy edit twisted mixed tut andcollapsed',\n",
       " 'view storm developed amp collapsed albertville evening wind damage reported valleywx',\n",
       " 'michaelgbaron panic city collapsed party city full swing',\n",
       " 'jessicalauraxo grandma collapsed died',\n",
       " 'inquiry launched collapsed hongkong furniture retailer dsc scmpnews',\n",
       " 'official crowd tent collapsed told stay put lancaster nh circus workers first told spectators',\n",
       " 'jemmaswansneeded breather today oh godwent lunch collapsed like sack bones car',\n",
       " 'room full people lifting collide prayer excited friday night',\n",
       " 'maybe someday well find place dreams reality collide',\n",
       " 'im saying freak collide starts mins freak get',\n",
       " 'added video youtube playlist howie day collide',\n",
       " 'even stars moon collidenever wantback life mtvhottest justin bieber',\n",
       " 'collide gateway students make setting chairs tables fun thanwouldve thought collidenlc',\n",
       " 'perez hilton forever niley shipper lol',\n",
       " 'escape place hide time space collide',\n",
       " 'nigga first timeheard song justine sky collide',\n",
       " 'vamos newells',\n",
       " 'funfact facts stars collide',\n",
       " 'transgender sam takes runaway lizzie two different worlds collide explore unjust boundaries trustsamuelbooks',\n",
       " 'social media marketing ethics rules collide recorder',\n",
       " 'trump amp bill clinton collide best conspiracy story ever via motherjones trump democratic plant lmao lastword',\n",
       " 'pierce veil rubber bracelet wristband collide sky full read ebay',\n",
       " 'theres days collide get signed',\n",
       " 'farhankvirk ptisalarkhan instead promoting false news first get facts right road rage incident vehicles collided',\n",
       " 'want win arc collided emery lords release open intl thestartofemandyou giveaway',\n",
       " 'mollywoodagreedidnt know moved marketplacewoke report amp thought favorite worlds bolapm collided',\n",
       " 'currently reading see collided dresser waiting get read',\n",
       " 'edsheeran tf innit',\n",
       " 'running stairs bad idea full collided floor',\n",
       " 'maritime sand carrier shinto maru chemical tanker hoshin maru collided nautical miles honshu',\n",
       " 'oh god sending thoughts sastars head coach dan hughes left game stretcher danielle robinson collided',\n",
       " 'remember beau andcollided slip slide anddied',\n",
       " 'ok two favorite things collided one great ht literarycakes',\n",
       " 'call car vs fed ex truck head collision mile marker prayers families get scene',\n",
       " 'motorcyclist bicyclist injured denver collision broadway',\n",
       " 'motorcyclist bicyclist injured denver collision broadway least two people taken local',\n",
       " 'traffic collision ambulance enroute florin rd franklin blvd south sac',\n",
       " 'denver post motorcyclist bicyclist injured denver collision broadway',\n",
       " 'collision daaamn',\n",
       " 'marin trfc collisionunkn inj',\n",
       " 'riverside con con trfc collisionunkn inj',\n",
       " 'techerit anti collision rear gadget technology',\n",
       " 'anti collision rear innovation gadgets',\n",
       " 'mom fights recover hospital fiery car crash kids miss home',\n",
       " 'citation billings police officer caused broadwater crash',\n",
       " 'little angel banjo pilon dies skateboarding crash wamberal weekend old',\n",
       " 'lol guy whipping double yellow line mustang crash curb light pole sns',\n",
       " 'new post india train crash kills dozens',\n",
       " 'crash',\n",
       " 'crashmatrix themercedesxxx minivan',\n",
       " 'nonpower schools best shot crash college football playoff',\n",
       " 'hilarious look got hairity rainbow crash spitty pie spikebot',\n",
       " 'thieves crash dealership gate steal four vehicles killeen',\n",
       " 'time enjoy summer crash course two weekswill never take summer classes thats lie',\n",
       " 'portmoody clarke rd hill nb blocked crash past seaview traffic back kemsley take como lake gatensbury get around',\n",
       " 'natethewolf car vs motorcycle crash fairport nine',\n",
       " 'learn cpanel join cpanel crash course',\n",
       " 'sterlingknight joeylawrence abcfmelissajoey car crash pretty fun film melissaandjoeychat',\n",
       " 'news injured school bus crash bordentown details',\n",
       " 'crash new depression seeds next crisis already sown',\n",
       " 'husbands back edinburgh crashed one young adults got raging temperature others gone ive tea',\n",
       " 'rip whoevers face crashed front work wishcould helped',\n",
       " 'almost crashed listening walk moon butdont carelove',\n",
       " 'olhead crashed back another olhead car hopped like damnnnnfuckin rookie slow tf',\n",
       " 'thomicks isnt showing methink crashed',\n",
       " 'rosannebarr new documents suggest clintons email server may crashed fan mail qatar must crashed',\n",
       " 'bin laden family plane crashed avoiding microlight landing far runway three members',\n",
       " 'blizzardcs crashed right stream lol',\n",
       " 'pahandabear nethaera yup eu crashed p',\n",
       " 'news bin laden family plane crashed avoiding microlight landing far runway three member',\n",
       " 'aqgco pakistan says army helicopter crashed countrys restive northwest fox news',\n",
       " 'got ticket amp car got crashed man lifes great',\n",
       " 'pak army helicopter crashed mansehra pray departed souls',\n",
       " 'website malfunctioning php scripts working database crashed need unique script php pro rescue',\n",
       " 'army personnel including majors martyred helicopter army medical corps crashed near mansehra',\n",
       " 'last days proven popular bbc watched bake iplayer crashed due cricket commentary',\n",
       " 'pkadlik jojowizphilipp andhave crush rob thomas right viptickets imgunnashakeitallnight',\n",
       " 'brother said crush girl named danielle oh god',\n",
       " 'women crush sure',\n",
       " 'thinkmight crush lover boy',\n",
       " 'aramis crush pitch strop',\n",
       " 'secretly crush sing lmk lmfao',\n",
       " 'crush cars wrestling stars mmx racing featuring wwe appadvice designgeeks',\n",
       " 'freakingg playing candy crush',\n",
       " 'im relationship person thats whenhave crush someonealways think theyre going feel way',\n",
       " 'dudehave crush hockey players paul always give butterflieshes angel',\n",
       " 'biggest girl crush oomf',\n",
       " 'teambarrowman saying man crush',\n",
       " 'omghave huge crush looks necessarilydont think looks way',\n",
       " 'crushed mi run pace nike sportwatch gps nikeplus',\n",
       " 'neptunia girls nearly level ive crushed nonsrank colosseum fights major grinding happens soon',\n",
       " 'amyramponi theyre crushed doesnt set negative selffulfilling prophecy negative selftalk psychat',\n",
       " 'old man shuffled slowly ice cream parlor ordered banana split waitress asked crushed nuts said arthritis',\n",
       " 'dancingonhwy nothinglove seeing bears fans get hopes dreams crushed',\n",
       " 'one episode undo season kai joins ff ren beats aichi psy misaki gets crushed fuck',\n",
       " 'rt target best price avril crushed sheer curtainpanel curtains decor blinds shades',\n",
       " 'also mcutchen crushed',\n",
       " 'im getting hopes crushed',\n",
       " 'uribe crushed',\n",
       " 'doublejust crushed ball jays',\n",
       " 'got season funniest season ever hilarious gameofthrones crushed omg cantstoplaughing wow comedy',\n",
       " 'soul crushed damn cute',\n",
       " 'loraccee joenbc msnbc still gets crushed foxnews says little cable news viewers general dumb sells sucks',\n",
       " 'uribe crushed mets',\n",
       " 'brandonwarne mlb ball feet absolutely crushed',\n",
       " 'ball crushed',\n",
       " 'big papi great welcoming show crushed',\n",
       " 'spicenthingsup wtfcant love bitreally think thats feels massive crush got crushed',\n",
       " 'ghazalabaji exactly curfew like situation whensit dad',\n",
       " 'black african refugees israel attend kenye west concert due curfew black refugees',\n",
       " 'kids bedtime gotten later amp later nows good time point regular curfew would btsprep parenting',\n",
       " 'karliknox know crazy strict philip gave texting curfew bruh',\n",
       " 'curfewbeagle beaglefreedom pretty curfew',\n",
       " 'parents try put curfew youre almost',\n",
       " 'everybody like fuck curfew im sitting like ill bed',\n",
       " 'benznibeadel hehe likehahaha im kidding loveim gon na sleep na bc curfew',\n",
       " 'annaesterly dude mins past curfew give self break',\n",
       " 'big hanging curfew',\n",
       " 'dometufnellpark know curfew havoc fest sunday please',\n",
       " 'sexthemed ebooks given curfew germany',\n",
       " 'big hanging curfew',\n",
       " 'happy report quite tropics cyclone development expected time cbsmiami miamiherald',\n",
       " 'comes next episode scorpion cyclone scorpioncbs',\n",
       " 'revival cyclone football begins today september',\n",
       " 'appeal cardinal charles bo sdb cnua dear brothers sisters recent cyclone massive',\n",
       " 'cyclone hits skye',\n",
       " 'orochinagicom gigatech cyclone',\n",
       " 'severe weather bulletin typhoon hannaph soudelortropical cyclone warningissued pm',\n",
       " 'first time everything coney island cyclone',\n",
       " 'aliyeskii wquddin maungkyawnu drkhubybe nslwin ibrahimdubashi mdskar zarnikyaw cyclone',\n",
       " 'erpestaraint bitch girl popobawa revolves around cyclone',\n",
       " 'weather tropicalweather tropical cyclone tracking update aug tropicalupdate',\n",
       " 'hurricane typhoon matter one also cyclone',\n",
       " 'waige driving like badass scorpionpilot cyclone scorpioncbs',\n",
       " 'raleigh cyclone gear mountain bike sportinggoods',\n",
       " 'rt severe weather bulletin typhoon soudelortropical cyclone warningissued pm',\n",
       " 'red tropical cyclone alert china northern mariana islands aug utc',\n",
       " 'localeventcountdown curtain opens black cyclone shelby tickets amp info rstoday',\n",
       " 'may rodder super stock keeling mustang barracuda riviera cyclone',\n",
       " 'damage done recover negative social media update smexaminer',\n",
       " 'beforeitsnews called damage control dont fooled folks perps come plan b',\n",
       " 'lms frauncestavern named one historic properties granted nygovcuomo repair sandy damage',\n",
       " 'globepolitics extreme positions lack debate pandering hardright increase divides corrodes trust damage goodwill',\n",
       " 'limited damage put runs board nyy',\n",
       " 'never riding gaymaro would damage reputation',\n",
       " 'alpine city park vandalized damage',\n",
       " 'got first damage today fuckkkkkk',\n",
       " 'vzwsupport zero damage horrible product',\n",
       " 'gatorgmuzik fettilootch slanglucci oppressions greatest danger coming soon album',\n",
       " 'used really think danger gon na movie',\n",
       " 'dont think realistic scully finally home sfe wanted go danger hipsbeforehands',\n",
       " 'onceaho empire girl vote abella danger adult empire running first ever hunt emp alwaysaho',\n",
       " 'free book blow mind suspense danger anger love importantly',\n",
       " 'post todaytalk dangers interruptions annoy',\n",
       " 'theelmagoo gopteens foxnews pattonoswalt anyone actually plays drinking game serious danger alcohol poisoning',\n",
       " 'pukesmoothiedont understand would put us danger',\n",
       " 'mlpdelrim please tell might danger knew knew',\n",
       " 'devil wears prada still one favourite films',\n",
       " 'allen west obama clear present danger republic',\n",
       " 'fresenius saidput patients lives danger time whats happening like bigger violation ppe',\n",
       " 'jamesmaynot amusicvideoaday kingruckus theres lot businesses operate outskirts ampnever feel danger',\n",
       " 'unitedutilities happyreceived text parasite dangeri transplantee amp virus kill kidney',\n",
       " 'last retweethopedrop dead',\n",
       " 'thought dead google glass gets second coming though considered epic failure according makers recent patent',\n",
       " 'dead',\n",
       " 'possum problems dead ducks',\n",
       " 'hope ross amp val thats dead emmerdale summerfate',\n",
       " 'dead ass truth pay shit gone use',\n",
       " 'emmerdale summerfate intense cant believe vals dead',\n",
       " 'nah emmerdale mikeparractor ross aint dead know hes unconscious dramatic teases',\n",
       " 'ross better dead emmerdale',\n",
       " 'nooooo village hotty dead emmerdale',\n",
       " 'mikeparractor im believing ross isnt dead',\n",
       " 'mikeparractor ross deadshall never watch emmerdale ill heartbroken',\n",
       " 'dont reckon ross dead actually said pete killed brother ross survived much die',\n",
       " 'please tell ross isnt actually deadi hope theyre playing us next episode emmerdale',\n",
       " 'flood two people dead houses destroyed kaduna',\n",
       " 'youre boyfriends ex feelings dont talk dont follow dead dont exist niggah',\n",
       " 'everyone doesnt know chris prater struck train morning pronounced dead scene prayers family',\n",
       " 'tonedidit cant real nah mother dead wrong',\n",
       " 'way ross dead emmerdale',\n",
       " 'incredible character heart broken actually dead ripross miss',\n",
       " 'baeffee randompelar black death black death whoo',\n",
       " 'liked youtube video camp bunkd disney channel',\n",
       " 'obama dont worry good iranians kidding death america chants bad ones like republicans',\n",
       " 'microsoft xbox console rrod red ring death parts repair full read',\n",
       " 'headache death',\n",
       " 'itristontyler confession whenwas littlehad hamster room cold froze death oops',\n",
       " 'wall going death medue years water leaks lack maintenance',\n",
       " 'iran deal result terrorism death truth revolt',\n",
       " 'swearyg looks like death star',\n",
       " 'death certainty life',\n",
       " 'single death tradery many deaths statistics',\n",
       " 'spirituality born againspiritual awakening near death experiences charlesfrize frizemedia dynamicfrize',\n",
       " 'kaaypaat actually death takes way longer youd think',\n",
       " 'days whenlook like deathhave see mo city chick fil today',\n",
       " 'deaths',\n",
       " 'betrayedhunter reading paused came across couple deaths began read deeper',\n",
       " 'deaths',\n",
       " 'bigamist first wife charged deaths second pregnant wife child mother',\n",
       " 'health fact women account smoking deaths',\n",
       " 'series deaths seemed one right another book seems month pass death liz',\n",
       " 'bootlegalbano shaunkingknow profits deaths people claims fighting makes sick',\n",
       " 'mohammedzismail deaths regular gunsmissilesbombs warfare abombs used war',\n",
       " 'infections driving neonatal deaths health minister guyana',\n",
       " 'bigamist first wife charged deaths second pregnant wife child mother nephew u',\n",
       " 'msha stepping enforcement mining deaths via elkodaily',\n",
       " 'bigamist first wife charged deaths second pregnant wife child mothe',\n",
       " 'wing part enough solve mystery',\n",
       " 'debris found indian ocean could malaysia flight',\n",
       " 'bbc news reunion debris missing malaysia flight',\n",
       " 'news island wreckage missing malaysian jet officials debris found reunion island flight',\n",
       " 'hopefully bring closure families malaysian pm confirms debris missing flight',\n",
       " 'runion debris almost surely flight officials say',\n",
       " 'aus aircraft debris found island malaysia confirms abc',\n",
       " 'aircraft debris found la reunion missing malaysia airlines',\n",
       " 'authorities increasingly confident airplane debris recovered wednesday comes official analysis set begin',\n",
       " 'investigators plane debris conclusively gadgets tech',\n",
       " 'top story reunion debris missing plane bbc news see',\n",
       " 'per malaysias pm piece debris conclusively part wing right',\n",
       " 'interesting aircraft debris found la reunion missing malaysia airlines abc please rt',\n",
       " 'rmt playing jackson browne deluge listeners requests',\n",
       " 'liquidslap name ja rules best song lt guess song ive never heard nothinghave heard could possibly candidate',\n",
       " 'already happens abs rowers tweeting support nats last year deluge celeb endorsements obama',\n",
       " 'ciostrategyau applicable business warfare battles future decided data',\n",
       " 'euro ensembles much optimistic weekend even though operational counterpart deluge',\n",
       " 'im havin previous life flashbacks whenlived weimar berlin hustlin life unter der linden deluge',\n",
       " 'bigdata deluge',\n",
       " 'decadelong billiondollar deluge messages government canada harperslegacy',\n",
       " 'feel deluged unhappiness take quiz',\n",
       " 'deluged low selfimage take quiz',\n",
       " 'thousands displaced houses amp roads deluged floodwater week persistent rains floods climate cdnpoli',\n",
       " 'businesses deluged invoices make stand colour shape anq likely rise top pay pile',\n",
       " 'businesses deluged invoices make stand colour shape likkly rise top os pay pile',\n",
       " 'police still searching two people rain mud deluged western town tetovo deutsche sprache',\n",
       " 'businesses cre deluged invoices make stand colour shape likely rise top pay cile',\n",
       " 'watching deluged data doczone ipad cbc',\n",
       " 'businesses deluged invoices make stand colour shape likely rise top che pay pile',\n",
       " 'businesses deluged invoices make yoursstand colour shape likely rise top pay pipe',\n",
       " 'feel deluged unhappiness take quiz',\n",
       " 'businesses adeluged invoices make standwout colour shape likely rise top pay pile',\n",
       " 'businesses deluged invoicesx make stand ou colour shape likely rise top pay pile',\n",
       " 'brain deluged thoughts',\n",
       " 'businesses deluged invoices make stand olour share likely rise top pay pile',\n",
       " 'businesses deluged invoicew make stand colouj shape likely rise top pay pile',\n",
       " 'businesses deluged invoices make stand colour shape ikely rise pay pile',\n",
       " 'cant believe never occurred thatcould deluged kickstarter emails',\n",
       " 'businesses deluged ivoices make stand colour shape likely ris top pay pile',\n",
       " 'facebook deluged friending spam last days someone targeting reason',\n",
       " 'glimpses ltbgthyderabadltbgt deluged heavy rainfall hyderabad flood waters heading',\n",
       " 'deluged dismay sosoon surrendered summer droughtdescending disturbedsunrisehaiku poetry wimberley atxsmtx',\n",
       " 'horror community deluged cruddy ostensibly macabre photography dont contribute pandemic',\n",
       " 'bartonleclay house owner told demolish extension bbc news',\n",
       " 'rt abbswinston zionist terrorist demolish tire repair shop structure bethlehem',\n",
       " 'salvation army bid demolish cottages southend',\n",
       " 'enugu government demolish illegal structures international conference centre',\n",
       " 'contractor hired demolish goodrich plant miami files bankruptcy tulsa world manufacturing',\n",
       " 'enugu government demolish illegal structures international conference centre enugu state government app',\n",
       " 'demolish head corruptwill vandalise head aliyu mani secschoolinnigeria',\n",
       " 'listen anakasparian demolish case planned parenthoodwhiny voicejust really love aboooooortiooooonnnnns defundpp',\n",
       " 'hope someone buys former post office napa sale preservation',\n",
       " 'demolish team right eat pizza dont need fitness ashton irwin mtvhottest',\n",
       " 'trying get higher bathroom work pen go demolish food',\n",
       " 'year old ring fort demolished',\n",
       " 'enugu government demolish illegal structures international conference centre rt',\n",
       " 'economic wisdom great economists demolish establishment nonsense case logic reason',\n",
       " 'shane im sorry wrong propagating attack would demolish england reversal',\n",
       " 'promise winningwhit andcould assemble team demolish',\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample_submission['target'] = np.array(predictions)\n",
    "sample_submission.to_csv('submission_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
